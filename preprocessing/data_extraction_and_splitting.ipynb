{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "按患者提取每一段preictal和interictal的数据信息\n",
        "preictal形如:[[('chb01_03.edf', (896.0, 2696.0))], [('chb01_14.edf', (3239.0, 3600)), ('chb01_15.edf', (0, 1432.0))], [('chb01_15.edf', (2522.0, 3600)), ('chb01_16.edf', (0, 715.0))], [('chb01_17.edf', (3227.0, 3600)), ('chb01_18.edf', (0, 1420.0))], [('chb01_20.edf', (1133.0, 2663)), ('chb01_21.edf', (0, 27.0))], [('chb01_25.edf', (3370.0, 3600)), ('chb01_26.edf', (0, 1562.0))]]\n",
        "interictal形如:[('chb01_08.edf', (1466.0, 3600)), ('chb01_09.edf', (0, 3600)), ('chb01_10.edf', (0, 3600)), ('chb01_11.edf', (0, 1762.0)), ('chb01_32.edf', (2553.0, 3600)), ('chb01_33.edf', (0, 3600)), ('chb01_34.edf', (0, 3600)), ('chb01_36.edf', (0, 3600)), ('chb01_37.edf', (0, 3600)), ('chb01_38.edf', (0, 3600)), ('chb01_39.edf', (0, 3600)), ('chb01_40.edf', (0, 3600)), ('chb01_41.edf', (0, 3600)), ('chb01_42.edf', (0, 3600)), ('chb01_43.edf', (0, 3600)), ('chb01_46.edf', (0, 3600))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['chb23_06.edf', 'chb23_07.edf', 'chb23_08.edf', 'chb23_09.edf', 'chb23_10.edf', 'chb23_16.edf', 'chb23_17.edf', 'chb23_19.edf', 'chb23_20.edf']\n",
            "[{'filename': 'chb23_06.edf', 'start_in_file': 3962.0, 'start_cumulative': 3962.0, 'end_in_file': 4075.0, 'end_cumulative': 4075.0, 'interval_cumulative': (3962.0, 4075.0)}, {'filename': 'chb23_08.edf', 'start_in_file': 325.0, 'start_cumulative': 10533.0, 'end_in_file': 345.0, 'end_cumulative': 10553.0, 'interval_cumulative': (10533.0, 10553.0)}, {'filename': 'chb23_08.edf', 'start_in_file': 5104.0, 'start_cumulative': 15312.0, 'end_in_file': 5151.0, 'end_cumulative': 15359.0, 'interval_cumulative': (15312.0, 15359.0)}, {'filename': 'chb23_09.edf', 'start_in_file': 2589.0, 'start_cumulative': 23159.0, 'end_in_file': 2660.0, 'end_cumulative': 23230.0, 'interval_cumulative': (23159.0, 23230.0)}, {'filename': 'chb23_09.edf', 'start_in_file': 6885.0, 'start_cumulative': 27455.0, 'end_in_file': 6947.0, 'end_cumulative': 27517.0, 'interval_cumulative': (27455.0, 27517.0)}, {'filename': 'chb23_09.edf', 'start_in_file': 8505.0, 'start_cumulative': 29075.0, 'end_in_file': 8532.0, 'end_cumulative': 29102.0, 'interval_cumulative': (29075.0, 29102.0)}, {'filename': 'chb23_09.edf', 'start_in_file': 9580.0, 'start_cumulative': 30150.0, 'end_in_file': 9664.0, 'end_cumulative': 30234.0, 'interval_cumulative': (30150.0, 30234.0)}]\n",
            "[{'filename': 'chb23_06.edf', 'start_in_file': 3962.0, 'start_cumulative': 3962.0, 'end_in_file': 4075.0, 'end_cumulative': 4075.0, 'interval_cumulative': (3962.0, 4075.0)}, {'filename': 'chb23_08.edf', 'start_in_file': 325.0, 'start_cumulative': 10533.0, 'end_in_file': 345.0, 'end_cumulative': 10553.0, 'interval_cumulative': (10533.0, 10553.0)}, {'filename': 'chb23_08.edf', 'start_in_file': 5104.0, 'start_cumulative': 15312.0, 'end_in_file': 5151.0, 'end_cumulative': 15359.0, 'interval_cumulative': (15312.0, 15359.0)}, {'filename': 'chb23_09.edf', 'start_in_file': 2589.0, 'start_cumulative': 23159.0, 'end_in_file': 2660.0, 'end_cumulative': 23230.0, 'interval_cumulative': (23159.0, 23230.0)}, {'filename': 'chb23_09.edf', 'start_in_file': 6885.0, 'start_cumulative': 27455.0, 'end_in_file': 6947.0, 'end_cumulative': 27517.0, 'interval_cumulative': (27455.0, 27517.0)}]\n",
            "3962.0\n",
            "0\n",
            "10533.0\n",
            "0\n",
            "15312.0\n",
            "0\n",
            "23159.0\n",
            "0\n",
            "27455.0\n",
            "0\n",
            "(3962.0, 4075.0)\n",
            "(10533.0, 10553.0)\n",
            "(15312.0, 15359.0)\n",
            "(23159.0, 23230.0)\n",
            "(27455.0, 27517.0)\n",
            "(29075.0, 29102.0)\n",
            "(30150.0, 30234.0)\n",
            "[(-10438.0, 18475.0), (-3867.0, 24953.0), (912.0, 29759.0), (8759.0, 37630.0), (13055.0, 41917.0), (14675.0, 43502.0), (15750.0, 44634.0)]\n",
            "[(-10438.0, 44634.0)]\n",
            "[(44634.0, 168848)]\n",
            "None\n",
            "None\n",
            "None\n",
            "None\n",
            "(44634.0, 49423)\n",
            "(103715, 118115)\n",
            "(118125, 130712)\n",
            "(149431, 163831)\n",
            "(163839, 168848)\n",
            "[[('chb23_06.edf', (1862.0, 3662.0))], [('chb23_07.edf', (914.0, 2560)), ('chb23_08.edf', (0, 25.0))], [('chb23_08.edf', (3004.0, 4804.0))], [('chb23_09.edf', (489.0, 2289.0))], [('chb23_09.edf', (4785.0, 6585.0))]]\n",
            "[('chb23_10.edf', (9611.0, 14400)), ('chb23_16.edf', (0, 14400)), ('chb23_17.edf', (0, 12587)), ('chb23_19.edf', (0, 14400)), ('chb23_20.edf', (0, 5009))]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "DATA_DIR = Path('D:\\陈教授组\\CHB-MIT')\n",
        "PATIENT_NUM = 23\n",
        "\n",
        "def tuple_add(t1, t2):\n",
        "    \"\"\"将两个元组的对应元素相加\"\"\"\n",
        "    return tuple(a + b for a, b in zip(t1, t2))\n",
        "\n",
        "def interval_difference(main_interval, remove_intervals):\n",
        "    \"\"\"\n",
        "    计算一个区间减去多个区间的差集\n",
        "    \n",
        "    Args:\n",
        "        main_interval: 主区间，格式 (start, end)\n",
        "        remove_intervals: 要移除的区间列表，格式 [(start, end), ...]\n",
        "    \n",
        "    Returns:\n",
        "        差集区间列表，可能为空、一个或多个区间\n",
        "    \"\"\"\n",
        "    if not remove_intervals:\n",
        "        return [main_interval]\n",
        "    \n",
        "    # 初始化结果为整个主区间\n",
        "    result = [main_interval]\n",
        "    \n",
        "    # 逐个移除每个 remove_interval\n",
        "    for remove_interval in remove_intervals:\n",
        "        new_result = []\n",
        "        for interval in result:\n",
        "            # 对当前结果中的每个区间，减去 remove_interval\n",
        "            parts = _subtract_two_intervals(interval, remove_interval)\n",
        "            new_result.extend(parts)  # 添加差集部分\n",
        "        result = new_result  # 更新结果\n",
        "    \n",
        "    # 可选：合并重叠或相邻的区间（如果 remove_intervals 有重叠）\n",
        "    # 这里我们先不合并，保留所有片段\n",
        "    return result\n",
        "\n",
        "def _subtract_two_intervals(interval1, interval2):\n",
        "    \"\"\"\n",
        "    计算两个区间之间的差集（私有辅助函数）\n",
        "    返回 interval1 - interval2 的结果（列表）\n",
        "    \"\"\"\n",
        "    start1, end1 = interval1\n",
        "    start2, end2 = interval2\n",
        "    \n",
        "    # 计算交集\n",
        "    inter_start = max(start1, start2)\n",
        "    inter_end   = min(end1, end2)\n",
        "    \n",
        "    # 如果无交集，返回原区间\n",
        "    if inter_start >= inter_end:\n",
        "        return [interval1]\n",
        "    \n",
        "    parts = []\n",
        "    if start1 < inter_start:\n",
        "        parts.append((start1, inter_start))\n",
        "    if inter_end < end1:\n",
        "        parts.append((inter_end, end1))\n",
        "    \n",
        "    return parts\n",
        "\n",
        "def interval_union(intervals):\n",
        "    \"\"\"\n",
        "    计算多个区间的并集\n",
        "    \n",
        "    Args:\n",
        "        intervals: 区间列表，每个区间为 (start, end) 元组\n",
        "    \n",
        "    Returns:\n",
        "        合并后的区间列表\n",
        "    \"\"\"\n",
        "    if not intervals:\n",
        "        return []\n",
        "    \n",
        "    # 1. 按起始时间排序\n",
        "    sorted_intervals = sorted(intervals, key=lambda x: x[0])\n",
        "    \n",
        "    # 2. 合并\n",
        "    union = [sorted_intervals[0]]  # 第一个区间\n",
        "    \n",
        "    for current in sorted_intervals[1:]:\n",
        "        last = union[-1]\n",
        "        \n",
        "        # 如果当前区间与上一个区间重叠或相邻\n",
        "        if current[0] <= last[1]:  # 有重叠\n",
        "            # 合并：更新上一个区间的结束时间为两者最大值\n",
        "            new_end = max(last[1], current[1])\n",
        "            union[-1] = (last[0], new_end)\n",
        "        else:\n",
        "            # 无重叠，直接添加\n",
        "            union.append(current)\n",
        "    \n",
        "    return union\n",
        "\n",
        "def interval_intersection_list(main_interval, intervals):\n",
        "    \"\"\"\n",
        "    计算一个区间与多个区间的交集\n",
        "    如果只有一个交集，返回元组；否则返回列表\n",
        "    \"\"\"\n",
        "    if not intervals:\n",
        "        return None  # 或者返回 []\n",
        "    \n",
        "    start1, end1 = main_interval\n",
        "    intersections = []\n",
        "    \n",
        "    for interval in intervals:\n",
        "        start2, end2 = interval\n",
        "        inter_start = max(start1, start2)\n",
        "        inter_end   = min(end1, end2)\n",
        "        \n",
        "        if inter_start < inter_end:\n",
        "            intersections.append((inter_start, inter_end))\n",
        "    \n",
        "    # 关键：根据数量决定返回类型\n",
        "    if len(intersections) == 0:\n",
        "        return None  # 无交集\n",
        "    elif len(intersections) == 1:\n",
        "        return intersections[0]  # 返回单个元组\n",
        "    else:\n",
        "        return intersections  # 返回列表\n",
        "\n",
        "def extract_preictals_and_interictals(summary_text: str, \n",
        "                                    preictal_duration_minutes: int = 35,\n",
        "                                    preictal_end_minutes: int = 5,\n",
        "                                    min_seizure_interval_minutes: int = 40,\n",
        "                                    excluded_time: int = 240) -> Dict:\n",
        "    \"\"\"\n",
        "    \n",
        "    Args:\n",
        "        summary_text: summary文件的文本内容\n",
        "        preictal_duration_minutes: 预发作期总时长（分钟），默认35分钟\n",
        "        preictal_end_minutes: 预发作期结束前多少分钟，默认5分钟（即35-5=30分钟有效）\n",
        "        min_seizure_interval_minutes: 最小发作间隔（分钟），默认40分钟\n",
        "    \n",
        "    Returns:\n",
        "        包含所有预发作期信息的字典\n",
        "    \"\"\"\n",
        "    \n",
        "    # 解析所有文件信息\n",
        "    files_info = parse_files_info_fixed(summary_text)\n",
        "    if not files_info:\n",
        "        return {}\n",
        "    \n",
        "    # 按文件顺序排序（不是按时间，因为文件本身就是按时间顺序的）\n",
        "    #files_info.sort(key=lambda x: x['filename'])\n",
        "    print([file_info['filename'] for file_info in files_info])\n",
        "    # 计算累积时间偏移\n",
        "    cumulative_offset = 0\n",
        "    for file_info in files_info:\n",
        "        file_info['cumulative_start'] = cumulative_offset + file_info['gap_before']\n",
        "        file_info['cumulative_end'] = file_info['cumulative_start'] + file_info['duration_seconds'] \n",
        "        file_info['cumulative_interval'] = (file_info['cumulative_start'], file_info['cumulative_end'])\n",
        "        cumulative_offset = file_info['cumulative_end']\n",
        "    total_interval = (0, files_info[-1]['cumulative_end'])\n",
        "    # 提取所有发作信息（使用累积时间）\n",
        "    preictal_segments = []\n",
        "    seizures = extract_seizures_with_cumulative_time(files_info,summary_text)\n",
        "    if not seizures:\n",
        "        return {}\n",
        "    \n",
        "    # 过滤间隔过短的发作\n",
        "    filtered_seizures = filter_seizures_by_interval_fixed(seizures, min_seizure_interval_minutes)\n",
        "    print(filtered_seizures)\n",
        "    \n",
        "    # 为每个有效发作生成预发作期\n",
        "    for i, seizure in enumerate(filtered_seizures, 1):\n",
        "        preictal_segments.append(generate_preictal_segments_fixed(\n",
        "            seizure, files_info, preictal_duration_minutes, preictal_end_minutes\n",
        "        ))\n",
        "\n",
        "    seizures_time_separate = []\n",
        "    for seizure in seizures:\n",
        "        print(seizure['interval_cumulative'])\n",
        "        seizures_time_separate.append(tuple_add(seizure['interval_cumulative'], (-excluded_time*60,excluded_time*60)))\n",
        "    print(seizures_time_separate)\n",
        "    seizures_time = interval_union(seizures_time_separate)\n",
        "    print(seizures_time)\n",
        "    interictals_time = interval_difference(total_interval, seizures_time)\n",
        "    print(interictals_time)\n",
        "    \n",
        "    interictal_segments = []\n",
        "    for file_info in files_info:\n",
        "        interical_interval = interval_intersection_list(file_info['cumulative_interval'], interictals_time)\n",
        "        print(interical_interval)\n",
        "        if interical_interval is not None:\n",
        "            interictal_segments.append((\n",
        "                file_info['filename'],\n",
        "                (interical_interval[0] - file_info['cumulative_start'], interical_interval[1] - file_info['cumulative_start'])\n",
        "            ))\n",
        "\n",
        "    return preictal_segments, interictal_segments\n",
        "\n",
        "def parse_time_to_seconds(time_str: str) -> int:\n",
        "    \"\"\"将时间字符串转换为秒数\"\"\"\n",
        "    try:\n",
        "        parts = time_str.split(':')\n",
        "        if len(parts) == 3:\n",
        "            hours, minutes, seconds = map(int, parts)\n",
        "            return hours * 3600 + minutes * 60 + seconds\n",
        "        elif len(parts) == 2:\n",
        "            minutes, seconds = map(int, parts)\n",
        "            return minutes * 60 + seconds\n",
        "        else:\n",
        "            return int(time_str)\n",
        "    except:\n",
        "        return 0\n",
        "        \n",
        "def parse_files_info_fixed(summary_text: str) -> List[Dict]:\n",
        "    \"\"\"解析所有文件的基本信息\"\"\"\n",
        "    files_info = []\n",
        "    lines = summary_text.strip().split('\\n')\n",
        "    \n",
        "    current_file = None\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        \n",
        "        if line.startswith('File Name:'):\n",
        "            current_file = line.split(':', 1)[1].strip()\n",
        "        elif line.startswith('File Start Time:') and current_file:\n",
        "            start_time_str = line.split(':', 1)[1].strip()\n",
        "        elif line.startswith('File End Time:') and current_file:\n",
        "            end_time_str = line.split(':', 1)[1].strip()\n",
        "            \n",
        "            # 计算文件时长（秒）\n",
        "            start_seconds = parse_time_to_seconds(start_time_str)\n",
        "            end_seconds = parse_time_to_seconds(end_time_str)\n",
        "            \n",
        "            # 处理跨夜情况\n",
        "            if end_seconds < start_seconds:\n",
        "                end_seconds += 24 * 3600  # 加24小时\n",
        "                        \n",
        "            files_info.append({\n",
        "                'filename': current_file,\n",
        "                'start_time_str': start_time_str,\n",
        "                'end_time_str': end_time_str,\n",
        "                'start_time_seconds': start_seconds,\n",
        "                'end_time_seconds': end_seconds,\n",
        "                'duration_seconds': end_seconds - start_seconds\n",
        "            })\n",
        "            current_file = None\n",
        "\n",
        "    files_info[0]['gap_before'] = 0\n",
        "            \n",
        "    for i in range(1, len(files_info)):\n",
        "        prev_file = files_info[i - 1]\n",
        "        curr_file = files_info[i]\n",
        "                \n",
        "        # 获取前一个文件的结束时间和当前文件的开始时间（真实时间轴）\n",
        "        prev_end_time = prev_file['end_time_seconds']\n",
        "        curr_start_time = curr_file['start_time_seconds']\n",
        "                \n",
        "        # 处理跨夜情况\n",
        "        if curr_start_time < prev_end_time:\n",
        "            curr_start_time += 24 * 3600  # 加24小时\n",
        "                \n",
        "        # 计算真实时间间隔（秒）\n",
        "        gap_seconds = curr_start_time - prev_end_time\n",
        "                \n",
        "        # 确保间隔非负\n",
        "        gap_seconds = max(0, gap_seconds)\n",
        "                \n",
        "        # 记录间隔\n",
        "        curr_file['gap_before'] = gap_seconds\n",
        "                  \n",
        "    return files_info\n",
        "\n",
        "\n",
        "def extract_seizures_with_cumulative_time(files_info: List[Dict], summary_text) -> List[Dict]:\n",
        "    \"\"\"从文件信息中提取所有发作，使用累积时间\"\"\"\n",
        "    seizures = []\n",
        "    lines = summary_text.strip().split('\\n')\n",
        "    \n",
        "    current_file = None\n",
        "    current_file_info = None\n",
        "    \n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        \n",
        "        if line.startswith('File Name:'):\n",
        "            current_file = line.split(':', 1)[1].strip()\n",
        "            current_file_info = next((f for f in files_info if f['filename'] == current_file), None)\n",
        "        elif 'Seizure' in line and 'Start Time:' in line and current_file and current_file_info:\n",
        "            try:\n",
        "                seizure_start_in_file = float(line.split(':', 1)[1].strip().split()[0])\n",
        "                seizure_start_cumulative = current_file_info['cumulative_start'] + seizure_start_in_file\n",
        "                seizures.append({\n",
        "                    'filename': current_file,\n",
        "                    'start_in_file': seizure_start_in_file,\n",
        "                    'start_cumulative': seizure_start_cumulative,\n",
        "                    'end_in_file': None,\n",
        "                    'end_cumulative': None\n",
        "                })\n",
        "            except (ValueError, IndexError):\n",
        "                continue\n",
        "        elif 'Seizure' in line and 'End Time:' in line and current_file and current_file_info and seizures:\n",
        "            try:\n",
        "                seizure_end_in_file = float(line.split(':', 1)[1].strip().split()[0])\n",
        "                seizure_end_cumulative = current_file_info['cumulative_start'] + seizure_end_in_file\n",
        "                seizures[-1]['end_in_file'] = seizure_end_in_file\n",
        "                seizures[-1]['end_cumulative'] = seizure_end_cumulative\n",
        "                seizures[-1]['interval_cumulative'] = (seizures[-1]['start_cumulative'], seizures[-1]['end_cumulative'])\n",
        "            except (ValueError, IndexError):\n",
        "                continue\n",
        "    print(seizures)\n",
        "    return seizures\n",
        "\n",
        "def filter_seizures_by_interval_fixed(seizures: List[Dict], min_interval_minutes: int) -> List[Dict]:\n",
        "    \"\"\"过滤间隔过短的发作\"\"\"\n",
        "    if not seizures:\n",
        "        return []\n",
        "    \n",
        "    filtered = [seizures[0]]  # 保留第一个发作\n",
        "    \n",
        "    for i in range(1, len(seizures)):\n",
        "        prev_seizure = seizures[i-1]\n",
        "        curr_seizure = seizures[i]\n",
        "        \n",
        "        # 计算间隔（分钟）\n",
        "        interval_minutes = (curr_seizure['start_cumulative'] - prev_seizure['end_cumulative']) / 60\n",
        "        \n",
        "        if interval_minutes >= min_interval_minutes:\n",
        "            filtered.append(curr_seizure)\n",
        "    \n",
        "    return filtered\n",
        "\n",
        "def interval_intersection(interval1: tuple, interval2: tuple) -> tuple:\n",
        "    \"\"\"\n",
        "    计算两个区间的交集\n",
        "    \n",
        "    Args:\n",
        "        interval1: 第一个区间，格式 (start, end)\n",
        "        interval2: 第二个区间，格式 (start, end)\n",
        "    \n",
        "    Returns:\n",
        "        交集区间 (start, end)，如果没有交集，返回 None 或空元组\n",
        "    \"\"\"\n",
        "    start1, end1 = interval1\n",
        "    start2, end2 = interval2\n",
        "    \n",
        "    # 计算交集的起始和结束\n",
        "    intersection_start = max(start1, start2)\n",
        "    intersection_end   = min(end1, end2)\n",
        "    \n",
        "    # 如果起始 >= 结束，说明无交集\n",
        "    if intersection_start < intersection_end:\n",
        "        return (intersection_start, intersection_end)\n",
        "    else:\n",
        "        return None  # 或者返回 ()\n",
        "\n",
        "def generate_preictal_segments_fixed(seizure: Dict, files_info: List[Dict], \n",
        "                                    preictal_duration_minutes: int, preictal_end_minutes: int) -> List[Tuple]:\n",
        "    \"\"\"为单个发作生成预发作期段（修复版）\"\"\"\n",
        "    segments = []\n",
        "    \n",
        "    # 计算预发作期的累积时间范围\n",
        "    seizure_start_cumulative = seizure['start_cumulative']\n",
        "    print(seizure_start_cumulative)\n",
        "    preictal_start_cumulative = seizure_start_cumulative - (preictal_duration_minutes * 60)\n",
        "    preictal_end_cumulative = seizure_start_cumulative - (preictal_end_minutes * 60)\n",
        "    \n",
        "    # 确保预发作期不早于第一个文件开始\n",
        "    first_file_start = files_info[0]['cumulative_start']\n",
        "    print(first_file_start)\n",
        "    if preictal_start_cumulative < first_file_start:\n",
        "        print('有数据在第一个文件之前。')\n",
        "        preictal_start_cumulative = first_file_start\n",
        "    \n",
        "    # 如果调整后的预发作期太短，则跳过\n",
        "    if preictal_end_cumulative <= preictal_start_cumulative:\n",
        "        print('无效数据。')\n",
        "        return segments\n",
        "    \n",
        "    preictal_time_interval = (preictal_start_cumulative,preictal_end_cumulative)\n",
        "\n",
        "    for file_info in files_info:\n",
        "        segment_interval = interval_intersection(file_info['cumulative_interval'],preictal_time_interval)\n",
        "        if segment_interval is not None:\n",
        "            segments.append((\n",
        "                file_info['filename'],\n",
        "                (segment_interval[0] - file_info['cumulative_start'], segment_interval[1] - file_info['cumulative_start'])\n",
        "            ))\n",
        "      \n",
        "    return segments\n",
        "\n",
        "# 测试修复版函数\n",
        "def data_splitting():\n",
        "    \"\"\"测试修复版预发作期提取\"\"\"\n",
        "    summary_path = DATA_DIR / f'chb{PATIENT_NUM:02d}' / f'chb{PATIENT_NUM:02d}-summary.txt'\n",
        "        \n",
        "    if summary_path.exists():\n",
        "        with open(summary_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            summary_text = f.read()\n",
        "            \n",
        "        preictals, interictals = extract_preictals_and_interictals(summary_text)\n",
        "        print(preictals)\n",
        "        print(interictals)\n",
        "        return preictals, interictals\n",
        "\n",
        "    else:\n",
        "        print(f\"未找到文件: {summary_path}\")\n",
        "\n",
        "# 运行测试\n",
        "preictals, interictals = data_splitting()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "根据上一步的数据信息提取实际的数据"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 开始处理 preictal (预发作期) 数据...\n",
            "\n",
            "正在处理第 1 个 preictal 段: chb23_06.edf (1862.0 - 3662.0s)\n",
            "  读取成功: 形状 (22, 460800), 时长 1800.0s, 保留 22 个通道\n",
            "------------------------------------------------------------\n",
            "正在处理第 2 个 preictal 段: chb23_07.edf (914.0 - 2560.0s)\n",
            "  读取成功: 形状 (22, 421376), 时长 1646.0s, 保留 22 个通道\n",
            "------------------------------------------------------------\n",
            "正在处理第 3 个 preictal 段: chb23_08.edf (0.0 - 25.0s)\n",
            "  读取成功: 形状 (22, 6400), 时长 25.0s, 保留 22 个通道\n",
            "------------------------------------------------------------\n",
            "正在处理第 4 个 preictal 段: chb23_08.edf (3004.0 - 4804.0s)\n",
            "  读取成功: 形状 (22, 460800), 时长 1800.0s, 保留 22 个通道\n",
            "------------------------------------------------------------\n",
            "正在处理第 5 个 preictal 段: chb23_09.edf (489.0 - 2289.0s)\n",
            "  读取成功: 形状 (22, 460800), 时长 1800.0s, 保留 22 个通道\n",
            "------------------------------------------------------------\n",
            "正在处理第 6 个 preictal 段: chb23_09.edf (4785.0 - 6585.0s)\n",
            "  读取成功: 形状 (22, 460800), 时长 1800.0s, 保留 22 个通道\n",
            "------------------------------------------------------------\n",
            "\n",
            "✅ 总共成功提取了 6 个独立的 preictal 段。\n",
            "\n",
            "🚀 开始处理 interictal (间歇期) 数据...\n",
            "\n",
            "正在处理第 1 个 interictal 段: chb23_10.edf (9611.0 - 14400.0s)\n",
            "  读取成功: 形状 (22, 1225984), 时长 4789.0s, 保留 22 个通道\n",
            "------------------------------------------------------------\n",
            "正在处理第 2 个 interictal 段: chb23_16.edf (0.0 - 14400.0s)\n",
            "  读取成功: 形状 (22, 3686400), 时长 14400.0s, 保留 22 个通道\n",
            "------------------------------------------------------------\n",
            "正在处理第 3 个 interictal 段: chb23_17.edf (0.0 - 12587.0s)\n",
            "  读取成功: 形状 (22, 3222272), 时长 12587.0s, 保留 22 个通道\n",
            "------------------------------------------------------------\n",
            "正在处理第 4 个 interictal 段: chb23_19.edf (0.0 - 14400.0s)\n",
            "  读取成功: 形状 (22, 3686400), 时长 14400.0s, 保留 22 个通道\n",
            "------------------------------------------------------------\n",
            "正在处理第 5 个 interictal 段: chb23_20.edf (0.0 - 5009.0s)\n",
            "  读取成功: 形状 (22, 1282304), 时长 5009.0s, 保留 22 个通道\n",
            "------------------------------------------------------------\n",
            "\n",
            "✅ 总共成功提取了 5 个独立的 interictal 段。\n",
            "\n",
            "============================================================\n",
            "✅ 所有数据处理完成！\n",
            "成功提取 preictal 段数: 6\n",
            "成功提取 interictal 段数: 5\n"
          ]
        }
      ],
      "source": [
        "import pyedflib\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Optional\n",
        "\n",
        "# 假设这是您提供的预发作期和间歇期信息\n",
        "preictal_segments = preictals\n",
        "interictal_segments = interictals\n",
        "\n",
        "# 数据集根目录 (请根据您的实际情况修改)\n",
        "DATA_ROOT = Path(f'D:\\陈教授组\\CHB-MIT\\chb{PATIENT_NUM:02d}')  # 注意使用 raw string 或双反斜杠\n",
        "\n",
        "# 定义您要保留的通道列表\n",
        "TARGET_CHANNELS = [\n",
        "    'C3-P3', 'C4-P4', 'CZ-PZ', 'F3-C3', 'F4-C4', 'F7-T7', 'F8-T8', 'FP1-F3', 'FP1-F7', 'FP2-F4',\n",
        "    'FP2-F8', 'FT10-T8', 'FT9-FT10', 'FZ-CZ', 'P3-O1', 'P4-O2', 'P7-O1', 'P7-T7', 'P8-O2', 'T7-FT9',\n",
        "    'T7-P7', 'T8-P8'\n",
        "]\n",
        "\n",
        "\n",
        "def read_edf_segment(\n",
        "    file_path: Path,\n",
        "    start_sec: float,\n",
        "    end_sec: float,\n",
        "    target_channels: List[str]\n",
        ") -> Tuple[Optional[np.ndarray], List[str], float, List[str]]:\n",
        "    \"\"\"\n",
        "    从EDF文件中读取指定时间段的数据，并筛选指定通道，去除重复项。\n",
        "\n",
        "    Args:\n",
        "        file_path: .edf 文件的路径\n",
        "        start_sec: 起始时间 (秒)\n",
        "        end_sec: 结束时间 (秒)\n",
        "        target_channels: 用户指定的目标通道列表。\n",
        "\n",
        "    Returns:\n",
        "        (data, final_channels, sample_rate, removed_channels)\n",
        "        data: 形状为 (n_channels, n_samples) 的numpy数组，如果未找到任何目标通道则为None\n",
        "        final_channels: 最终保留的通道名称列表（已去重）\n",
        "        sample_rate: 采样率 (Hz)\n",
        "        removed_channels: 被移除的重复通道列表\n",
        "    \"\"\"\n",
        "    with pyedflib.EdfReader(str(file_path)) as f:\n",
        "        sample_rate = f.getSampleFrequency(0)\n",
        "        all_file_channels = f.getSignalLabels()\n",
        "\n",
        "        # 找出文件中存在且在目标列表中的通道\n",
        "        common_channels = [ch for ch in target_channels if ch in all_file_channels]\n",
        "\n",
        "        if not common_channels:\n",
        "            print(f\"  文件 {file_path.name} 中未找到任何目标通道。\")\n",
        "            return None, [], sample_rate, []\n",
        "\n",
        "        # 去重，保持顺序\n",
        "        seen = set()\n",
        "        final_channels = []\n",
        "        removed_duplicates = []\n",
        "        for ch in common_channels:\n",
        "            if ch in seen:\n",
        "                removed_duplicates.append(ch)\n",
        "            else:\n",
        "                seen.add(ch)\n",
        "                final_channels.append(ch)\n",
        "\n",
        "        if removed_duplicates:\n",
        "            print(f\"  在文件 {file_path.name} 中检测到并移除了重复通道: {removed_duplicates}\")\n",
        "\n",
        "        # 计算样本索引\n",
        "        start_sample = int(start_sec * sample_rate)\n",
        "        end_sample = int(end_sec * sample_rate)\n",
        "\n",
        "        # 读取数据\n",
        "        data = np.vstack([\n",
        "            f.readSignal(all_file_channels.index(ch), start_sample, end_sample - start_sample)\n",
        "            for ch in final_channels\n",
        "        ])\n",
        "\n",
        "        return data, final_channels, sample_rate, removed_duplicates\n",
        "\n",
        "\n",
        "def process_segments(segment_list: List, data_label: str):\n",
        "    \"\"\"\n",
        "    处理 preictal 或 interictal 段。\n",
        "    支持两种结构：\n",
        "        - preictal: [[('f1',(s,e)), ('f2',(s,e))], ...] → 每个元组是一段\n",
        "        - interictal: [('f1',(s,e)), ('f2',(s,e)), ...] → 每个元组是一段\n",
        "\n",
        "    所有段独立处理，不合并。\n",
        "\n",
        "    Returns:\n",
        "        all_data: 每个有效段都作为一个独立项加入结果列表\n",
        "    \"\"\"\n",
        "    all_data = []\n",
        "    global_segment_index = 0  # 用于全局编号\n",
        "\n",
        "    for i, segment in enumerate(segment_list):\n",
        "        # 统一展开成 file-time 对列表\n",
        "        if isinstance(segment, list):\n",
        "            if len(segment) == 0:\n",
        "                continue\n",
        "            if isinstance(segment[0], tuple) and isinstance(segment[0][1], tuple):\n",
        "                # 情况1: segment 是列表，如 [('f1', (s,e)), ('f2', (s,e))]\n",
        "                file_time_pairs = segment\n",
        "            else:\n",
        "                continue\n",
        "        elif isinstance(segment, tuple) and isinstance(segment[1], tuple):\n",
        "            # 情况2: segment 是单个元组，如 ('f1', (s,e))\n",
        "            file_time_pairs = [segment]\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        # 现在 file_time_pairs 是 [(), (), ...]，每个元组是一段\n",
        "        for file_name, (start_sec, end_sec) in file_time_pairs:\n",
        "            global_segment_index += 1\n",
        "            print(f\"正在处理第 {global_segment_index} 个 {data_label} 段: {file_name} ({start_sec:.1f} - {end_sec:.1f}s)\")\n",
        "\n",
        "            file_path = DATA_ROOT / file_name\n",
        "            if not file_path.exists():\n",
        "                print(f\"  警告: 文件 {file_path} 不存在，跳过...\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                data_segment, channels, sample_rate, removed = read_edf_segment(\n",
        "                    file_path, start_sec, end_sec, TARGET_CHANNELS\n",
        "                )\n",
        "\n",
        "                if data_segment is None:\n",
        "                    print(f\"  文件 {file_name} 中无有效目标通道，跳过此时间段。\")\n",
        "                    continue\n",
        "\n",
        "                duration = data_segment.shape[1] / sample_rate\n",
        "\n",
        "                print(f\"  读取成功: 形状 {data_segment.shape}, 时长 {duration:.1f}s, \"\n",
        "                      f\"保留 {len(channels)} 个通道\")\n",
        "                if removed:\n",
        "                    print(f\"  移除重复通道: {list(set(removed))}\")\n",
        "\n",
        "                all_data.append({\n",
        "                    'data': data_segment,\n",
        "                    'channels': channels,\n",
        "                    'sample_rate': sample_rate,\n",
        "                    'duration_seconds': duration,\n",
        "                    'file': file_name,\n",
        "                    'start_sec': start_sec,\n",
        "                    'end_sec': end_sec\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  读取 {file_name} 时出错: {e}\")\n",
        "                continue\n",
        "\n",
        "            print(\"-\" * 60)\n",
        "\n",
        "    print(f\"\\n✅ 总共成功提取了 {len(all_data)} 个独立的 {data_label} 段。\\n\")\n",
        "    return all_data\n",
        "\n",
        "\n",
        "# ========================\n",
        "# 主处理流程\n",
        "# ========================\n",
        "\n",
        "# 处理 preictal 数据（完全保留原逻辑）\n",
        "print(\"🚀 开始处理 preictal (预发作期) 数据...\\n\")\n",
        "all_preictal_data = process_segments(preictal_segments, \"preictal\")\n",
        "\n",
        "# 处理 interictal 数据（新增）\n",
        "print(\"🚀 开始处理 interictal (间歇期) 数据...\\n\")\n",
        "all_interictal_data = process_segments(interictal_segments, \"interictal\")\n",
        "\n",
        "# 最终结果\n",
        "print(\"=\" * 60)\n",
        "print(\"✅ 所有数据处理完成！\")\n",
        "print(f\"成功提取 preictal 段数: {len(all_preictal_data)}\")\n",
        "print(f\"成功提取 interictal 段数: {len(all_interictal_data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "通过公式计算preictal的滑动窗口步长，进行样本分割（5s），使得preictal数据量和interictal相似，解决数据不平衡问题"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "preictal文件 chb23_06.edf 形状：(719, 22, 1280)\n",
            "preictal文件 chb23_07.edf 形状：(657, 22, 1280)\n",
            "preictal文件 chb23_08.edf 形状：(9, 22, 1280)\n",
            "preictal文件 chb23_08.edf 形状：(719, 22, 1280)\n",
            "preictal文件 chb23_09.edf 形状：(719, 22, 1280)\n",
            "preictal文件 chb23_09.edf 形状：(719, 22, 1280)\n",
            "所有 preictal 段总样本数: 3542\n",
            "interictal文件 chb23_10.edf 形状：(957, 22, 1280)\n",
            "interictal文件 chb23_16.edf 形状：(2880, 22, 1280)\n",
            "interictal文件 chb23_17.edf 形状：(2517, 22, 1280)\n",
            "interictal文件 chb23_19.edf 形状：(2880, 22, 1280)\n",
            "interictal文件 chb23_20.edf 形状：(1001, 22, 1280)\n",
            "所有 interictal 段总样本数: 10235\n",
            "Patient 23 数据已保存为 HDF5 格式：\n",
            "  Preictal: D:\\陈教授组\\mymodel\\data\\unprocessed\\preictal\\preictal_fragments23.h5\n",
            "  Interictal: D:\\陈教授组\\mymodel\\data\\unprocessed\\interictal\\interictal_fragments23.h5\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# 设置采样窗口和通道数\n",
        "S = 5  # 秒\n",
        "CHANNEL_NUM = 22  # 通道数\n",
        "\n",
        "# 统计 preictal 和 interictal 的总时长（秒），合并所有段\n",
        "M = sum([d['duration_seconds'] for d in all_preictal_data])\n",
        "N = sum([d['duration_seconds'] for d in all_interictal_data])\n",
        "\n",
        "# 计算比例 K\n",
        "K = M / N if N > 0 else 1\n",
        "'''\n",
        "print(f\"所有 preictal 段总时长 M: {M:.2f} 秒\")\n",
        "print(f\"所有 interictal 段总时长 N: {N:.2f} 秒\")\n",
        "print(f\"窗口长度 S: {S} 秒\")\n",
        "print(f\"步长比例 K: {K:.4f}\")\n",
        "'''\n",
        "K_DETERMINED = 0.5  # 强制设置 K=0.5\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# 确保保存目录存在\n",
        "save_dir = f\"D:\\\\陈教授组\\\\mymodel\\\\data\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# 构建保存路径（使用 .h5 扩展名）\n",
        "save_path_preictal = f\"{save_dir}\\\\unprocessed\\\\preictal\\\\preictal_fragments{PATIENT_NUM:02d}.h5\"\n",
        "save_path_interictal = f\"{save_dir}\\\\unprocessed\\\\interictal\\\\interictal_fragments{PATIENT_NUM:02d}.h5\"\n",
        "\n",
        "# 分割并保存 preictal 片段\n",
        "with h5py.File(save_path_preictal, 'w') as f:\n",
        "    total_preictal_sample = 0\n",
        "    for seg_idx, seg in enumerate(all_preictal_data, 1):\n",
        "        preictal_list = []\n",
        "        data = seg['data']  # shape: (channels, timepoints)\n",
        "        sample_rate = seg['sample_rate']\n",
        "        total_points = data.shape[1]\n",
        "        window_points = int(S * sample_rate)\n",
        "        step_points = int(S * K_DETERMINED * sample_rate)\n",
        "        # 只保留前22通道\n",
        "        data = data[:CHANNEL_NUM, :]\n",
        "        # 计算片段数\n",
        "        a = int(np.floor((data.shape[1] - window_points) / step_points) + 1)\n",
        "        total_preictal_sample += a\n",
        "        for i in range(a):\n",
        "            start = i * step_points\n",
        "            end = start + window_points\n",
        "            if end > total_points:\n",
        "                break\n",
        "            frag = data[:, start:end]\n",
        "            preictal_list.append(frag)\n",
        "        preictal_fragments = np.stack(preictal_list, axis = 0)\n",
        "        f.create_dataset(f'fragment_{seg_idx:02d}', data=preictal_fragments, compression='gzip')\n",
        "        print(f\"preictal文件 {seg['file']} 形状：{preictal_fragments.shape}\")\n",
        "    print(f\"所有 preictal 段总样本数: {total_preictal_sample}\")\n",
        "\n",
        "# 分割 interictal 段\n",
        "with h5py.File(save_path_interictal, 'w') as f:\n",
        "    total_interictal_sample = 0\n",
        "    for seg_idx, seg in enumerate(all_interictal_data, 1):\n",
        "        interictal_list = []\n",
        "        data = seg['data']\n",
        "        sample_rate = seg['sample_rate']\n",
        "        total_points = data.shape[1]\n",
        "        window_points = int(S * sample_rate)\n",
        "        step_points = int(S * sample_rate)\n",
        "        data = data[:CHANNEL_NUM, :]\n",
        "        b = int(np.floor((data.shape[1] - window_points) / step_points) + 1)\n",
        "        total_interictal_sample += b\n",
        "        for i in range(b):\n",
        "            start = i * step_points\n",
        "            end = start + window_points\n",
        "            if end > total_points:\n",
        "                break\n",
        "            frag = data[:, start:end]\n",
        "            interictal_list.append(frag)\n",
        "        interictal_fragments = np.stack(interictal_list, axis = 0)\n",
        "        f.create_dataset(f'fragment_{seg_idx:02d}', data=interictal_fragments, compression='gzip')\n",
        "        print(f\"interictal文件 {seg['file']} 形状：{interictal_fragments.shape}\")\n",
        "    print(f\"所有 interictal 段总样本数: {total_interictal_sample}\")\n",
        "\n",
        "print(f\"Patient {PATIENT_NUM:02d} 数据已保存为 HDF5 格式：\")\n",
        "print(f\"  Preictal: {save_path_preictal}\")\n",
        "print(f\"  Interictal: {save_path_interictal}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "series_list = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "data = pd.Series({\n",
        "    'Patient_Number': PATIENT_NUM,\n",
        "    'Preictal_Time_Duration': round(M,2) ,\n",
        "    'Interictal_Time_Duration': round(N,2),\n",
        "    'Theoretical_Step_Length_Ratio': round(K,4),\n",
        "    'Determined_Step_Length_Ratio': K_DETERMINED,\n",
        "    'Total_Preictal_Samples': total_preictal_sample,\n",
        "    'Total_Interictal_Samples': total_interictal_sample\n",
        "})\n",
        "series_list.append(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "患者数据统计表：\n",
            "====================================================================================================\n",
            " Patient_Number  Preictal_Time_Duration  Interictal_Time_Duration  Theoretical_Step_Length_Ratio  Determined_Step_Length_Ratio  Total_Preictal_Samples  Total_Interictal_Samples\n",
            "         1.0000              10528.0000                51743.0000                         0.2035                        0.5000               4197.0000                10347.0000\n",
            "         2.0000               5400.0000                93759.0000                         0.0576                        0.5000               2157.0000                18751.0000\n",
            "         3.0000              10358.0000                98620.0000                         0.1050                        0.5000               4131.0000                19723.0000\n",
            "         4.0000               3179.0000               422020.0000                         0.0075                        0.5000               1269.0000                84398.0000\n",
            "         5.0000               8987.0000                52019.0000                         0.1728                        0.5000               3585.0000                10403.0000\n",
            "         6.0000              17605.0000                89269.0000                         0.1972                        0.5000               7027.0000                17851.0000\n",
            "         7.0000               5400.0000               184756.0000                         0.0292                        0.5000               2157.0000                36949.0000\n",
            "         8.0000               8992.0000                 3623.0000                         2.4819                        0.5000               3590.0000                  724.0000\n",
            "         9.0000               7200.0000               166899.0000                         0.0431                        0.5000               2876.0000                33376.0000\n",
            "        10.0000              11883.0000                87395.0000                         0.1360                        0.5000               4746.0000                17478.0000\n",
            "        11.0000               2954.0000               111600.0000                         0.0265                        0.5000               1179.0000                22320.0000\n",
            "        14.0000              10335.0000                16967.0000                         0.6091                        0.5000               4123.0000                 3393.0000\n",
            "        16.0000               8979.0000                20318.0000                         0.4419                        0.5000               3581.0000                 4063.0000\n",
            "        17.0000               5400.0000                42494.0000                         0.1271                        0.5000               2157.0000                 8497.0000\n",
            "        18.0000               7158.0000                91570.0000                         0.0782                        0.5000               2857.0000                18314.0000\n",
            "        19.0000               3600.0000                93600.0000                         0.0385                        0.5000               1438.0000                18720.0000\n",
            "        20.0000               8279.0000                68651.0000                         0.1206                        0.5000               3303.0000                13728.0000\n",
            "        21.0000               7096.0000                84365.0000                         0.0841                        0.5000               2833.0000                16872.0000\n",
            "        22.0000               4563.0000                61419.0000                         0.0743                        0.5000               1822.0000                12282.0000\n",
            "        23.0000               8871.0000                51185.0000                         0.1733                        0.5000               3542.0000                10235.0000\n",
            "====================================================================================================\n",
            "\n",
            "表格已保存至: D:\\陈教授组\\mymodel\\data\\patient_statistics.csv\n",
            "\n",
            "总计处理患者数: 20\n",
            "总Preictal样本数: 62570\n",
            "总Interictal样本数: 378424\n"
          ]
        }
      ],
      "source": [
        "# 将所有 Series 转换为 DataFrame 表格\n",
        "data_table = pd.concat(series_list, axis=1).T\n",
        "data_table.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# 显示表格\n",
        "print(\"患者数据统计表：\")\n",
        "print(\"=\" * 100)\n",
        "print(data_table.to_string(index=False, float_format='%.4f'))\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# 也可以保存为 CSV 文件\n",
        "csv_path = f\"D:\\\\陈教授组\\\\mymodel\\\\data\\\\patient_statistics.csv\"\n",
        "data_table.to_csv(csv_path, index=False, float_format='%.4f')\n",
        "print(f\"\\n表格已保存至: {csv_path}\")\n",
        "\n",
        "# 显示一些统计信息\n",
        "print(f\"\\n总计处理患者数: {len(data_table)}\")\n",
        "print(f\"总Preictal样本数: {data_table['Total_Preictal_Samples'].sum():.0f}\")\n",
        "print(f\"总Interictal样本数: {data_table['Total_Interictal_Samples'].sum():.0f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
