{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "æŒ‰æ‚£è€…æå–æ¯ä¸€æ®µpreictalå’Œinterictalçš„æ•°æ®ä¿¡æ¯\n",
        "preictalå½¢å¦‚:[[('chb01_03.edf', (896.0, 2696.0))], [('chb01_14.edf', (3239.0, 3600)), ('chb01_15.edf', (0, 1432.0))], [('chb01_15.edf', (2522.0, 3600)), ('chb01_16.edf', (0, 715.0))], [('chb01_17.edf', (3227.0, 3600)), ('chb01_18.edf', (0, 1420.0))], [('chb01_20.edf', (1133.0, 2663)), ('chb01_21.edf', (0, 27.0))], [('chb01_25.edf', (3370.0, 3600)), ('chb01_26.edf', (0, 1562.0))]]\n",
        "interictalå½¢å¦‚:[('chb01_08.edf', (1466.0, 3600)), ('chb01_09.edf', (0, 3600)), ('chb01_10.edf', (0, 3600)), ('chb01_11.edf', (0, 1762.0)), ('chb01_32.edf', (2553.0, 3600)), ('chb01_33.edf', (0, 3600)), ('chb01_34.edf', (0, 3600)), ('chb01_36.edf', (0, 3600)), ('chb01_37.edf', (0, 3600)), ('chb01_38.edf', (0, 3600)), ('chb01_39.edf', (0, 3600)), ('chb01_40.edf', (0, 3600)), ('chb01_41.edf', (0, 3600)), ('chb01_42.edf', (0, 3600)), ('chb01_43.edf', (0, 3600)), ('chb01_46.edf', (0, 3600))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['chb23_06.edf', 'chb23_07.edf', 'chb23_08.edf', 'chb23_09.edf', 'chb23_10.edf', 'chb23_16.edf', 'chb23_17.edf', 'chb23_19.edf', 'chb23_20.edf']\n",
            "[{'filename': 'chb23_06.edf', 'start_in_file': 3962.0, 'start_cumulative': 3962.0, 'end_in_file': 4075.0, 'end_cumulative': 4075.0, 'interval_cumulative': (3962.0, 4075.0)}, {'filename': 'chb23_08.edf', 'start_in_file': 325.0, 'start_cumulative': 10533.0, 'end_in_file': 345.0, 'end_cumulative': 10553.0, 'interval_cumulative': (10533.0, 10553.0)}, {'filename': 'chb23_08.edf', 'start_in_file': 5104.0, 'start_cumulative': 15312.0, 'end_in_file': 5151.0, 'end_cumulative': 15359.0, 'interval_cumulative': (15312.0, 15359.0)}, {'filename': 'chb23_09.edf', 'start_in_file': 2589.0, 'start_cumulative': 23159.0, 'end_in_file': 2660.0, 'end_cumulative': 23230.0, 'interval_cumulative': (23159.0, 23230.0)}, {'filename': 'chb23_09.edf', 'start_in_file': 6885.0, 'start_cumulative': 27455.0, 'end_in_file': 6947.0, 'end_cumulative': 27517.0, 'interval_cumulative': (27455.0, 27517.0)}, {'filename': 'chb23_09.edf', 'start_in_file': 8505.0, 'start_cumulative': 29075.0, 'end_in_file': 8532.0, 'end_cumulative': 29102.0, 'interval_cumulative': (29075.0, 29102.0)}, {'filename': 'chb23_09.edf', 'start_in_file': 9580.0, 'start_cumulative': 30150.0, 'end_in_file': 9664.0, 'end_cumulative': 30234.0, 'interval_cumulative': (30150.0, 30234.0)}]\n",
            "[{'filename': 'chb23_06.edf', 'start_in_file': 3962.0, 'start_cumulative': 3962.0, 'end_in_file': 4075.0, 'end_cumulative': 4075.0, 'interval_cumulative': (3962.0, 4075.0)}, {'filename': 'chb23_08.edf', 'start_in_file': 325.0, 'start_cumulative': 10533.0, 'end_in_file': 345.0, 'end_cumulative': 10553.0, 'interval_cumulative': (10533.0, 10553.0)}, {'filename': 'chb23_08.edf', 'start_in_file': 5104.0, 'start_cumulative': 15312.0, 'end_in_file': 5151.0, 'end_cumulative': 15359.0, 'interval_cumulative': (15312.0, 15359.0)}, {'filename': 'chb23_09.edf', 'start_in_file': 2589.0, 'start_cumulative': 23159.0, 'end_in_file': 2660.0, 'end_cumulative': 23230.0, 'interval_cumulative': (23159.0, 23230.0)}, {'filename': 'chb23_09.edf', 'start_in_file': 6885.0, 'start_cumulative': 27455.0, 'end_in_file': 6947.0, 'end_cumulative': 27517.0, 'interval_cumulative': (27455.0, 27517.0)}]\n",
            "3962.0\n",
            "0\n",
            "10533.0\n",
            "0\n",
            "15312.0\n",
            "0\n",
            "23159.0\n",
            "0\n",
            "27455.0\n",
            "0\n",
            "(3962.0, 4075.0)\n",
            "(10533.0, 10553.0)\n",
            "(15312.0, 15359.0)\n",
            "(23159.0, 23230.0)\n",
            "(27455.0, 27517.0)\n",
            "(29075.0, 29102.0)\n",
            "(30150.0, 30234.0)\n",
            "[(-10438.0, 18475.0), (-3867.0, 24953.0), (912.0, 29759.0), (8759.0, 37630.0), (13055.0, 41917.0), (14675.0, 43502.0), (15750.0, 44634.0)]\n",
            "[(-10438.0, 44634.0)]\n",
            "[(44634.0, 168848)]\n",
            "None\n",
            "None\n",
            "None\n",
            "None\n",
            "(44634.0, 49423)\n",
            "(103715, 118115)\n",
            "(118125, 130712)\n",
            "(149431, 163831)\n",
            "(163839, 168848)\n",
            "[[('chb23_06.edf', (1862.0, 3662.0))], [('chb23_07.edf', (914.0, 2560)), ('chb23_08.edf', (0, 25.0))], [('chb23_08.edf', (3004.0, 4804.0))], [('chb23_09.edf', (489.0, 2289.0))], [('chb23_09.edf', (4785.0, 6585.0))]]\n",
            "[('chb23_10.edf', (9611.0, 14400)), ('chb23_16.edf', (0, 14400)), ('chb23_17.edf', (0, 12587)), ('chb23_19.edf', (0, 14400)), ('chb23_20.edf', (0, 5009))]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "DATA_DIR = Path('D:\\é™ˆæ•™æˆç»„\\CHB-MIT')\n",
        "PATIENT_NUM = 23\n",
        "\n",
        "def tuple_add(t1, t2):\n",
        "    \"\"\"å°†ä¸¤ä¸ªå…ƒç»„çš„å¯¹åº”å…ƒç´ ç›¸åŠ \"\"\"\n",
        "    return tuple(a + b for a, b in zip(t1, t2))\n",
        "\n",
        "def interval_difference(main_interval, remove_intervals):\n",
        "    \"\"\"\n",
        "    è®¡ç®—ä¸€ä¸ªåŒºé—´å‡å»å¤šä¸ªåŒºé—´çš„å·®é›†\n",
        "    \n",
        "    Args:\n",
        "        main_interval: ä¸»åŒºé—´ï¼Œæ ¼å¼ (start, end)\n",
        "        remove_intervals: è¦ç§»é™¤çš„åŒºé—´åˆ—è¡¨ï¼Œæ ¼å¼ [(start, end), ...]\n",
        "    \n",
        "    Returns:\n",
        "        å·®é›†åŒºé—´åˆ—è¡¨ï¼Œå¯èƒ½ä¸ºç©ºã€ä¸€ä¸ªæˆ–å¤šä¸ªåŒºé—´\n",
        "    \"\"\"\n",
        "    if not remove_intervals:\n",
        "        return [main_interval]\n",
        "    \n",
        "    # åˆå§‹åŒ–ç»“æœä¸ºæ•´ä¸ªä¸»åŒºé—´\n",
        "    result = [main_interval]\n",
        "    \n",
        "    # é€ä¸ªç§»é™¤æ¯ä¸ª remove_interval\n",
        "    for remove_interval in remove_intervals:\n",
        "        new_result = []\n",
        "        for interval in result:\n",
        "            # å¯¹å½“å‰ç»“æœä¸­çš„æ¯ä¸ªåŒºé—´ï¼Œå‡å» remove_interval\n",
        "            parts = _subtract_two_intervals(interval, remove_interval)\n",
        "            new_result.extend(parts)  # æ·»åŠ å·®é›†éƒ¨åˆ†\n",
        "        result = new_result  # æ›´æ–°ç»“æœ\n",
        "    \n",
        "    # å¯é€‰ï¼šåˆå¹¶é‡å æˆ–ç›¸é‚»çš„åŒºé—´ï¼ˆå¦‚æœ remove_intervals æœ‰é‡å ï¼‰\n",
        "    # è¿™é‡Œæˆ‘ä»¬å…ˆä¸åˆå¹¶ï¼Œä¿ç•™æ‰€æœ‰ç‰‡æ®µ\n",
        "    return result\n",
        "\n",
        "def _subtract_two_intervals(interval1, interval2):\n",
        "    \"\"\"\n",
        "    è®¡ç®—ä¸¤ä¸ªåŒºé—´ä¹‹é—´çš„å·®é›†ï¼ˆç§æœ‰è¾…åŠ©å‡½æ•°ï¼‰\n",
        "    è¿”å› interval1 - interval2 çš„ç»“æœï¼ˆåˆ—è¡¨ï¼‰\n",
        "    \"\"\"\n",
        "    start1, end1 = interval1\n",
        "    start2, end2 = interval2\n",
        "    \n",
        "    # è®¡ç®—äº¤é›†\n",
        "    inter_start = max(start1, start2)\n",
        "    inter_end   = min(end1, end2)\n",
        "    \n",
        "    # å¦‚æœæ— äº¤é›†ï¼Œè¿”å›åŸåŒºé—´\n",
        "    if inter_start >= inter_end:\n",
        "        return [interval1]\n",
        "    \n",
        "    parts = []\n",
        "    if start1 < inter_start:\n",
        "        parts.append((start1, inter_start))\n",
        "    if inter_end < end1:\n",
        "        parts.append((inter_end, end1))\n",
        "    \n",
        "    return parts\n",
        "\n",
        "def interval_union(intervals):\n",
        "    \"\"\"\n",
        "    è®¡ç®—å¤šä¸ªåŒºé—´çš„å¹¶é›†\n",
        "    \n",
        "    Args:\n",
        "        intervals: åŒºé—´åˆ—è¡¨ï¼Œæ¯ä¸ªåŒºé—´ä¸º (start, end) å…ƒç»„\n",
        "    \n",
        "    Returns:\n",
        "        åˆå¹¶åçš„åŒºé—´åˆ—è¡¨\n",
        "    \"\"\"\n",
        "    if not intervals:\n",
        "        return []\n",
        "    \n",
        "    # 1. æŒ‰èµ·å§‹æ—¶é—´æ’åº\n",
        "    sorted_intervals = sorted(intervals, key=lambda x: x[0])\n",
        "    \n",
        "    # 2. åˆå¹¶\n",
        "    union = [sorted_intervals[0]]  # ç¬¬ä¸€ä¸ªåŒºé—´\n",
        "    \n",
        "    for current in sorted_intervals[1:]:\n",
        "        last = union[-1]\n",
        "        \n",
        "        # å¦‚æœå½“å‰åŒºé—´ä¸ä¸Šä¸€ä¸ªåŒºé—´é‡å æˆ–ç›¸é‚»\n",
        "        if current[0] <= last[1]:  # æœ‰é‡å \n",
        "            # åˆå¹¶ï¼šæ›´æ–°ä¸Šä¸€ä¸ªåŒºé—´çš„ç»“æŸæ—¶é—´ä¸ºä¸¤è€…æœ€å¤§å€¼\n",
        "            new_end = max(last[1], current[1])\n",
        "            union[-1] = (last[0], new_end)\n",
        "        else:\n",
        "            # æ— é‡å ï¼Œç›´æ¥æ·»åŠ \n",
        "            union.append(current)\n",
        "    \n",
        "    return union\n",
        "\n",
        "def interval_intersection_list(main_interval, intervals):\n",
        "    \"\"\"\n",
        "    è®¡ç®—ä¸€ä¸ªåŒºé—´ä¸å¤šä¸ªåŒºé—´çš„äº¤é›†\n",
        "    å¦‚æœåªæœ‰ä¸€ä¸ªäº¤é›†ï¼Œè¿”å›å…ƒç»„ï¼›å¦åˆ™è¿”å›åˆ—è¡¨\n",
        "    \"\"\"\n",
        "    if not intervals:\n",
        "        return None  # æˆ–è€…è¿”å› []\n",
        "    \n",
        "    start1, end1 = main_interval\n",
        "    intersections = []\n",
        "    \n",
        "    for interval in intervals:\n",
        "        start2, end2 = interval\n",
        "        inter_start = max(start1, start2)\n",
        "        inter_end   = min(end1, end2)\n",
        "        \n",
        "        if inter_start < inter_end:\n",
        "            intersections.append((inter_start, inter_end))\n",
        "    \n",
        "    # å…³é”®ï¼šæ ¹æ®æ•°é‡å†³å®šè¿”å›ç±»å‹\n",
        "    if len(intersections) == 0:\n",
        "        return None  # æ— äº¤é›†\n",
        "    elif len(intersections) == 1:\n",
        "        return intersections[0]  # è¿”å›å•ä¸ªå…ƒç»„\n",
        "    else:\n",
        "        return intersections  # è¿”å›åˆ—è¡¨\n",
        "\n",
        "def extract_preictals_and_interictals(summary_text: str, \n",
        "                                    preictal_duration_minutes: int = 35,\n",
        "                                    preictal_end_minutes: int = 5,\n",
        "                                    min_seizure_interval_minutes: int = 40,\n",
        "                                    excluded_time: int = 240) -> Dict:\n",
        "    \"\"\"\n",
        "    \n",
        "    Args:\n",
        "        summary_text: summaryæ–‡ä»¶çš„æ–‡æœ¬å†…å®¹\n",
        "        preictal_duration_minutes: é¢„å‘ä½œæœŸæ€»æ—¶é•¿ï¼ˆåˆ†é’Ÿï¼‰ï¼Œé»˜è®¤35åˆ†é’Ÿ\n",
        "        preictal_end_minutes: é¢„å‘ä½œæœŸç»“æŸå‰å¤šå°‘åˆ†é’Ÿï¼Œé»˜è®¤5åˆ†é’Ÿï¼ˆå³35-5=30åˆ†é’Ÿæœ‰æ•ˆï¼‰\n",
        "        min_seizure_interval_minutes: æœ€å°å‘ä½œé—´éš”ï¼ˆåˆ†é’Ÿï¼‰ï¼Œé»˜è®¤40åˆ†é’Ÿ\n",
        "    \n",
        "    Returns:\n",
        "        åŒ…å«æ‰€æœ‰é¢„å‘ä½œæœŸä¿¡æ¯çš„å­—å…¸\n",
        "    \"\"\"\n",
        "    \n",
        "    # è§£ææ‰€æœ‰æ–‡ä»¶ä¿¡æ¯\n",
        "    files_info = parse_files_info_fixed(summary_text)\n",
        "    if not files_info:\n",
        "        return {}\n",
        "    \n",
        "    # æŒ‰æ–‡ä»¶é¡ºåºæ’åºï¼ˆä¸æ˜¯æŒ‰æ—¶é—´ï¼Œå› ä¸ºæ–‡ä»¶æœ¬èº«å°±æ˜¯æŒ‰æ—¶é—´é¡ºåºçš„ï¼‰\n",
        "    #files_info.sort(key=lambda x: x['filename'])\n",
        "    print([file_info['filename'] for file_info in files_info])\n",
        "    # è®¡ç®—ç´¯ç§¯æ—¶é—´åç§»\n",
        "    cumulative_offset = 0\n",
        "    for file_info in files_info:\n",
        "        file_info['cumulative_start'] = cumulative_offset + file_info['gap_before']\n",
        "        file_info['cumulative_end'] = file_info['cumulative_start'] + file_info['duration_seconds'] \n",
        "        file_info['cumulative_interval'] = (file_info['cumulative_start'], file_info['cumulative_end'])\n",
        "        cumulative_offset = file_info['cumulative_end']\n",
        "    total_interval = (0, files_info[-1]['cumulative_end'])\n",
        "    # æå–æ‰€æœ‰å‘ä½œä¿¡æ¯ï¼ˆä½¿ç”¨ç´¯ç§¯æ—¶é—´ï¼‰\n",
        "    preictal_segments = []\n",
        "    seizures = extract_seizures_with_cumulative_time(files_info,summary_text)\n",
        "    if not seizures:\n",
        "        return {}\n",
        "    \n",
        "    # è¿‡æ»¤é—´éš”è¿‡çŸ­çš„å‘ä½œ\n",
        "    filtered_seizures = filter_seizures_by_interval_fixed(seizures, min_seizure_interval_minutes)\n",
        "    print(filtered_seizures)\n",
        "    \n",
        "    # ä¸ºæ¯ä¸ªæœ‰æ•ˆå‘ä½œç”Ÿæˆé¢„å‘ä½œæœŸ\n",
        "    for i, seizure in enumerate(filtered_seizures, 1):\n",
        "        preictal_segments.append(generate_preictal_segments_fixed(\n",
        "            seizure, files_info, preictal_duration_minutes, preictal_end_minutes\n",
        "        ))\n",
        "\n",
        "    seizures_time_separate = []\n",
        "    for seizure in seizures:\n",
        "        print(seizure['interval_cumulative'])\n",
        "        seizures_time_separate.append(tuple_add(seizure['interval_cumulative'], (-excluded_time*60,excluded_time*60)))\n",
        "    print(seizures_time_separate)\n",
        "    seizures_time = interval_union(seizures_time_separate)\n",
        "    print(seizures_time)\n",
        "    interictals_time = interval_difference(total_interval, seizures_time)\n",
        "    print(interictals_time)\n",
        "    \n",
        "    interictal_segments = []\n",
        "    for file_info in files_info:\n",
        "        interical_interval = interval_intersection_list(file_info['cumulative_interval'], interictals_time)\n",
        "        print(interical_interval)\n",
        "        if interical_interval is not None:\n",
        "            interictal_segments.append((\n",
        "                file_info['filename'],\n",
        "                (interical_interval[0] - file_info['cumulative_start'], interical_interval[1] - file_info['cumulative_start'])\n",
        "            ))\n",
        "\n",
        "    return preictal_segments, interictal_segments\n",
        "\n",
        "def parse_time_to_seconds(time_str: str) -> int:\n",
        "    \"\"\"å°†æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸ºç§’æ•°\"\"\"\n",
        "    try:\n",
        "        parts = time_str.split(':')\n",
        "        if len(parts) == 3:\n",
        "            hours, minutes, seconds = map(int, parts)\n",
        "            return hours * 3600 + minutes * 60 + seconds\n",
        "        elif len(parts) == 2:\n",
        "            minutes, seconds = map(int, parts)\n",
        "            return minutes * 60 + seconds\n",
        "        else:\n",
        "            return int(time_str)\n",
        "    except:\n",
        "        return 0\n",
        "        \n",
        "def parse_files_info_fixed(summary_text: str) -> List[Dict]:\n",
        "    \"\"\"è§£ææ‰€æœ‰æ–‡ä»¶çš„åŸºæœ¬ä¿¡æ¯\"\"\"\n",
        "    files_info = []\n",
        "    lines = summary_text.strip().split('\\n')\n",
        "    \n",
        "    current_file = None\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        \n",
        "        if line.startswith('File Name:'):\n",
        "            current_file = line.split(':', 1)[1].strip()\n",
        "        elif line.startswith('File Start Time:') and current_file:\n",
        "            start_time_str = line.split(':', 1)[1].strip()\n",
        "        elif line.startswith('File End Time:') and current_file:\n",
        "            end_time_str = line.split(':', 1)[1].strip()\n",
        "            \n",
        "            # è®¡ç®—æ–‡ä»¶æ—¶é•¿ï¼ˆç§’ï¼‰\n",
        "            start_seconds = parse_time_to_seconds(start_time_str)\n",
        "            end_seconds = parse_time_to_seconds(end_time_str)\n",
        "            \n",
        "            # å¤„ç†è·¨å¤œæƒ…å†µ\n",
        "            if end_seconds < start_seconds:\n",
        "                end_seconds += 24 * 3600  # åŠ 24å°æ—¶\n",
        "                        \n",
        "            files_info.append({\n",
        "                'filename': current_file,\n",
        "                'start_time_str': start_time_str,\n",
        "                'end_time_str': end_time_str,\n",
        "                'start_time_seconds': start_seconds,\n",
        "                'end_time_seconds': end_seconds,\n",
        "                'duration_seconds': end_seconds - start_seconds\n",
        "            })\n",
        "            current_file = None\n",
        "\n",
        "    files_info[0]['gap_before'] = 0\n",
        "            \n",
        "    for i in range(1, len(files_info)):\n",
        "        prev_file = files_info[i - 1]\n",
        "        curr_file = files_info[i]\n",
        "                \n",
        "        # è·å–å‰ä¸€ä¸ªæ–‡ä»¶çš„ç»“æŸæ—¶é—´å’Œå½“å‰æ–‡ä»¶çš„å¼€å§‹æ—¶é—´ï¼ˆçœŸå®æ—¶é—´è½´ï¼‰\n",
        "        prev_end_time = prev_file['end_time_seconds']\n",
        "        curr_start_time = curr_file['start_time_seconds']\n",
        "                \n",
        "        # å¤„ç†è·¨å¤œæƒ…å†µ\n",
        "        if curr_start_time < prev_end_time:\n",
        "            curr_start_time += 24 * 3600  # åŠ 24å°æ—¶\n",
        "                \n",
        "        # è®¡ç®—çœŸå®æ—¶é—´é—´éš”ï¼ˆç§’ï¼‰\n",
        "        gap_seconds = curr_start_time - prev_end_time\n",
        "                \n",
        "        # ç¡®ä¿é—´éš”éè´Ÿ\n",
        "        gap_seconds = max(0, gap_seconds)\n",
        "                \n",
        "        # è®°å½•é—´éš”\n",
        "        curr_file['gap_before'] = gap_seconds\n",
        "                  \n",
        "    return files_info\n",
        "\n",
        "\n",
        "def extract_seizures_with_cumulative_time(files_info: List[Dict], summary_text) -> List[Dict]:\n",
        "    \"\"\"ä»æ–‡ä»¶ä¿¡æ¯ä¸­æå–æ‰€æœ‰å‘ä½œï¼Œä½¿ç”¨ç´¯ç§¯æ—¶é—´\"\"\"\n",
        "    seizures = []\n",
        "    lines = summary_text.strip().split('\\n')\n",
        "    \n",
        "    current_file = None\n",
        "    current_file_info = None\n",
        "    \n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        \n",
        "        if line.startswith('File Name:'):\n",
        "            current_file = line.split(':', 1)[1].strip()\n",
        "            current_file_info = next((f for f in files_info if f['filename'] == current_file), None)\n",
        "        elif 'Seizure' in line and 'Start Time:' in line and current_file and current_file_info:\n",
        "            try:\n",
        "                seizure_start_in_file = float(line.split(':', 1)[1].strip().split()[0])\n",
        "                seizure_start_cumulative = current_file_info['cumulative_start'] + seizure_start_in_file\n",
        "                seizures.append({\n",
        "                    'filename': current_file,\n",
        "                    'start_in_file': seizure_start_in_file,\n",
        "                    'start_cumulative': seizure_start_cumulative,\n",
        "                    'end_in_file': None,\n",
        "                    'end_cumulative': None\n",
        "                })\n",
        "            except (ValueError, IndexError):\n",
        "                continue\n",
        "        elif 'Seizure' in line and 'End Time:' in line and current_file and current_file_info and seizures:\n",
        "            try:\n",
        "                seizure_end_in_file = float(line.split(':', 1)[1].strip().split()[0])\n",
        "                seizure_end_cumulative = current_file_info['cumulative_start'] + seizure_end_in_file\n",
        "                seizures[-1]['end_in_file'] = seizure_end_in_file\n",
        "                seizures[-1]['end_cumulative'] = seizure_end_cumulative\n",
        "                seizures[-1]['interval_cumulative'] = (seizures[-1]['start_cumulative'], seizures[-1]['end_cumulative'])\n",
        "            except (ValueError, IndexError):\n",
        "                continue\n",
        "    print(seizures)\n",
        "    return seizures\n",
        "\n",
        "def filter_seizures_by_interval_fixed(seizures: List[Dict], min_interval_minutes: int) -> List[Dict]:\n",
        "    \"\"\"è¿‡æ»¤é—´éš”è¿‡çŸ­çš„å‘ä½œ\"\"\"\n",
        "    if not seizures:\n",
        "        return []\n",
        "    \n",
        "    filtered = [seizures[0]]  # ä¿ç•™ç¬¬ä¸€ä¸ªå‘ä½œ\n",
        "    \n",
        "    for i in range(1, len(seizures)):\n",
        "        prev_seizure = seizures[i-1]\n",
        "        curr_seizure = seizures[i]\n",
        "        \n",
        "        # è®¡ç®—é—´éš”ï¼ˆåˆ†é’Ÿï¼‰\n",
        "        interval_minutes = (curr_seizure['start_cumulative'] - prev_seizure['end_cumulative']) / 60\n",
        "        \n",
        "        if interval_minutes >= min_interval_minutes:\n",
        "            filtered.append(curr_seizure)\n",
        "    \n",
        "    return filtered\n",
        "\n",
        "def interval_intersection(interval1: tuple, interval2: tuple) -> tuple:\n",
        "    \"\"\"\n",
        "    è®¡ç®—ä¸¤ä¸ªåŒºé—´çš„äº¤é›†\n",
        "    \n",
        "    Args:\n",
        "        interval1: ç¬¬ä¸€ä¸ªåŒºé—´ï¼Œæ ¼å¼ (start, end)\n",
        "        interval2: ç¬¬äºŒä¸ªåŒºé—´ï¼Œæ ¼å¼ (start, end)\n",
        "    \n",
        "    Returns:\n",
        "        äº¤é›†åŒºé—´ (start, end)ï¼Œå¦‚æœæ²¡æœ‰äº¤é›†ï¼Œè¿”å› None æˆ–ç©ºå…ƒç»„\n",
        "    \"\"\"\n",
        "    start1, end1 = interval1\n",
        "    start2, end2 = interval2\n",
        "    \n",
        "    # è®¡ç®—äº¤é›†çš„èµ·å§‹å’Œç»“æŸ\n",
        "    intersection_start = max(start1, start2)\n",
        "    intersection_end   = min(end1, end2)\n",
        "    \n",
        "    # å¦‚æœèµ·å§‹ >= ç»“æŸï¼Œè¯´æ˜æ— äº¤é›†\n",
        "    if intersection_start < intersection_end:\n",
        "        return (intersection_start, intersection_end)\n",
        "    else:\n",
        "        return None  # æˆ–è€…è¿”å› ()\n",
        "\n",
        "def generate_preictal_segments_fixed(seizure: Dict, files_info: List[Dict], \n",
        "                                    preictal_duration_minutes: int, preictal_end_minutes: int) -> List[Tuple]:\n",
        "    \"\"\"ä¸ºå•ä¸ªå‘ä½œç”Ÿæˆé¢„å‘ä½œæœŸæ®µï¼ˆä¿®å¤ç‰ˆï¼‰\"\"\"\n",
        "    segments = []\n",
        "    \n",
        "    # è®¡ç®—é¢„å‘ä½œæœŸçš„ç´¯ç§¯æ—¶é—´èŒƒå›´\n",
        "    seizure_start_cumulative = seizure['start_cumulative']\n",
        "    print(seizure_start_cumulative)\n",
        "    preictal_start_cumulative = seizure_start_cumulative - (preictal_duration_minutes * 60)\n",
        "    preictal_end_cumulative = seizure_start_cumulative - (preictal_end_minutes * 60)\n",
        "    \n",
        "    # ç¡®ä¿é¢„å‘ä½œæœŸä¸æ—©äºç¬¬ä¸€ä¸ªæ–‡ä»¶å¼€å§‹\n",
        "    first_file_start = files_info[0]['cumulative_start']\n",
        "    print(first_file_start)\n",
        "    if preictal_start_cumulative < first_file_start:\n",
        "        print('æœ‰æ•°æ®åœ¨ç¬¬ä¸€ä¸ªæ–‡ä»¶ä¹‹å‰ã€‚')\n",
        "        preictal_start_cumulative = first_file_start\n",
        "    \n",
        "    # å¦‚æœè°ƒæ•´åçš„é¢„å‘ä½œæœŸå¤ªçŸ­ï¼Œåˆ™è·³è¿‡\n",
        "    if preictal_end_cumulative <= preictal_start_cumulative:\n",
        "        print('æ— æ•ˆæ•°æ®ã€‚')\n",
        "        return segments\n",
        "    \n",
        "    preictal_time_interval = (preictal_start_cumulative,preictal_end_cumulative)\n",
        "\n",
        "    for file_info in files_info:\n",
        "        segment_interval = interval_intersection(file_info['cumulative_interval'],preictal_time_interval)\n",
        "        if segment_interval is not None:\n",
        "            segments.append((\n",
        "                file_info['filename'],\n",
        "                (segment_interval[0] - file_info['cumulative_start'], segment_interval[1] - file_info['cumulative_start'])\n",
        "            ))\n",
        "      \n",
        "    return segments\n",
        "\n",
        "# æµ‹è¯•ä¿®å¤ç‰ˆå‡½æ•°\n",
        "def data_splitting():\n",
        "    \"\"\"æµ‹è¯•ä¿®å¤ç‰ˆé¢„å‘ä½œæœŸæå–\"\"\"\n",
        "    summary_path = DATA_DIR / f'chb{PATIENT_NUM:02d}' / f'chb{PATIENT_NUM:02d}-summary.txt'\n",
        "        \n",
        "    if summary_path.exists():\n",
        "        with open(summary_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            summary_text = f.read()\n",
        "            \n",
        "        preictals, interictals = extract_preictals_and_interictals(summary_text)\n",
        "        print(preictals)\n",
        "        print(interictals)\n",
        "        return preictals, interictals\n",
        "\n",
        "    else:\n",
        "        print(f\"æœªæ‰¾åˆ°æ–‡ä»¶: {summary_path}\")\n",
        "\n",
        "# è¿è¡Œæµ‹è¯•\n",
        "preictals, interictals = data_splitting()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "æ ¹æ®ä¸Šä¸€æ­¥çš„æ•°æ®ä¿¡æ¯æå–å®é™…çš„æ•°æ®"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ å¼€å§‹å¤„ç† preictal (é¢„å‘ä½œæœŸ) æ•°æ®...\n",
            "\n",
            "æ­£åœ¨å¤„ç†ç¬¬ 1 ä¸ª preictal æ®µ: chb23_06.edf (1862.0 - 3662.0s)\n",
            "  è¯»å–æˆåŠŸ: å½¢çŠ¶ (22, 460800), æ—¶é•¿ 1800.0s, ä¿ç•™ 22 ä¸ªé€šé“\n",
            "------------------------------------------------------------\n",
            "æ­£åœ¨å¤„ç†ç¬¬ 2 ä¸ª preictal æ®µ: chb23_07.edf (914.0 - 2560.0s)\n",
            "  è¯»å–æˆåŠŸ: å½¢çŠ¶ (22, 421376), æ—¶é•¿ 1646.0s, ä¿ç•™ 22 ä¸ªé€šé“\n",
            "------------------------------------------------------------\n",
            "æ­£åœ¨å¤„ç†ç¬¬ 3 ä¸ª preictal æ®µ: chb23_08.edf (0.0 - 25.0s)\n",
            "  è¯»å–æˆåŠŸ: å½¢çŠ¶ (22, 6400), æ—¶é•¿ 25.0s, ä¿ç•™ 22 ä¸ªé€šé“\n",
            "------------------------------------------------------------\n",
            "æ­£åœ¨å¤„ç†ç¬¬ 4 ä¸ª preictal æ®µ: chb23_08.edf (3004.0 - 4804.0s)\n",
            "  è¯»å–æˆåŠŸ: å½¢çŠ¶ (22, 460800), æ—¶é•¿ 1800.0s, ä¿ç•™ 22 ä¸ªé€šé“\n",
            "------------------------------------------------------------\n",
            "æ­£åœ¨å¤„ç†ç¬¬ 5 ä¸ª preictal æ®µ: chb23_09.edf (489.0 - 2289.0s)\n",
            "  è¯»å–æˆåŠŸ: å½¢çŠ¶ (22, 460800), æ—¶é•¿ 1800.0s, ä¿ç•™ 22 ä¸ªé€šé“\n",
            "------------------------------------------------------------\n",
            "æ­£åœ¨å¤„ç†ç¬¬ 6 ä¸ª preictal æ®µ: chb23_09.edf (4785.0 - 6585.0s)\n",
            "  è¯»å–æˆåŠŸ: å½¢çŠ¶ (22, 460800), æ—¶é•¿ 1800.0s, ä¿ç•™ 22 ä¸ªé€šé“\n",
            "------------------------------------------------------------\n",
            "\n",
            "âœ… æ€»å…±æˆåŠŸæå–äº† 6 ä¸ªç‹¬ç«‹çš„ preictal æ®µã€‚\n",
            "\n",
            "ğŸš€ å¼€å§‹å¤„ç† interictal (é—´æ­‡æœŸ) æ•°æ®...\n",
            "\n",
            "æ­£åœ¨å¤„ç†ç¬¬ 1 ä¸ª interictal æ®µ: chb23_10.edf (9611.0 - 14400.0s)\n",
            "  è¯»å–æˆåŠŸ: å½¢çŠ¶ (22, 1225984), æ—¶é•¿ 4789.0s, ä¿ç•™ 22 ä¸ªé€šé“\n",
            "------------------------------------------------------------\n",
            "æ­£åœ¨å¤„ç†ç¬¬ 2 ä¸ª interictal æ®µ: chb23_16.edf (0.0 - 14400.0s)\n",
            "  è¯»å–æˆåŠŸ: å½¢çŠ¶ (22, 3686400), æ—¶é•¿ 14400.0s, ä¿ç•™ 22 ä¸ªé€šé“\n",
            "------------------------------------------------------------\n",
            "æ­£åœ¨å¤„ç†ç¬¬ 3 ä¸ª interictal æ®µ: chb23_17.edf (0.0 - 12587.0s)\n",
            "  è¯»å–æˆåŠŸ: å½¢çŠ¶ (22, 3222272), æ—¶é•¿ 12587.0s, ä¿ç•™ 22 ä¸ªé€šé“\n",
            "------------------------------------------------------------\n",
            "æ­£åœ¨å¤„ç†ç¬¬ 4 ä¸ª interictal æ®µ: chb23_19.edf (0.0 - 14400.0s)\n",
            "  è¯»å–æˆåŠŸ: å½¢çŠ¶ (22, 3686400), æ—¶é•¿ 14400.0s, ä¿ç•™ 22 ä¸ªé€šé“\n",
            "------------------------------------------------------------\n",
            "æ­£åœ¨å¤„ç†ç¬¬ 5 ä¸ª interictal æ®µ: chb23_20.edf (0.0 - 5009.0s)\n",
            "  è¯»å–æˆåŠŸ: å½¢çŠ¶ (22, 1282304), æ—¶é•¿ 5009.0s, ä¿ç•™ 22 ä¸ªé€šé“\n",
            "------------------------------------------------------------\n",
            "\n",
            "âœ… æ€»å…±æˆåŠŸæå–äº† 5 ä¸ªç‹¬ç«‹çš„ interictal æ®µã€‚\n",
            "\n",
            "============================================================\n",
            "âœ… æ‰€æœ‰æ•°æ®å¤„ç†å®Œæˆï¼\n",
            "æˆåŠŸæå– preictal æ®µæ•°: 6\n",
            "æˆåŠŸæå– interictal æ®µæ•°: 5\n"
          ]
        }
      ],
      "source": [
        "import pyedflib\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Optional\n",
        "\n",
        "# å‡è®¾è¿™æ˜¯æ‚¨æä¾›çš„é¢„å‘ä½œæœŸå’Œé—´æ­‡æœŸä¿¡æ¯\n",
        "preictal_segments = preictals\n",
        "interictal_segments = interictals\n",
        "\n",
        "# æ•°æ®é›†æ ¹ç›®å½• (è¯·æ ¹æ®æ‚¨çš„å®é™…æƒ…å†µä¿®æ”¹)\n",
        "DATA_ROOT = Path(f'D:\\é™ˆæ•™æˆç»„\\CHB-MIT\\chb{PATIENT_NUM:02d}')  # æ³¨æ„ä½¿ç”¨ raw string æˆ–åŒåæ–œæ \n",
        "\n",
        "# å®šä¹‰æ‚¨è¦ä¿ç•™çš„é€šé“åˆ—è¡¨\n",
        "TARGET_CHANNELS = [\n",
        "    'C3-P3', 'C4-P4', 'CZ-PZ', 'F3-C3', 'F4-C4', 'F7-T7', 'F8-T8', 'FP1-F3', 'FP1-F7', 'FP2-F4',\n",
        "    'FP2-F8', 'FT10-T8', 'FT9-FT10', 'FZ-CZ', 'P3-O1', 'P4-O2', 'P7-O1', 'P7-T7', 'P8-O2', 'T7-FT9',\n",
        "    'T7-P7', 'T8-P8'\n",
        "]\n",
        "\n",
        "\n",
        "def read_edf_segment(\n",
        "    file_path: Path,\n",
        "    start_sec: float,\n",
        "    end_sec: float,\n",
        "    target_channels: List[str]\n",
        ") -> Tuple[Optional[np.ndarray], List[str], float, List[str]]:\n",
        "    \"\"\"\n",
        "    ä»EDFæ–‡ä»¶ä¸­è¯»å–æŒ‡å®šæ—¶é—´æ®µçš„æ•°æ®ï¼Œå¹¶ç­›é€‰æŒ‡å®šé€šé“ï¼Œå»é™¤é‡å¤é¡¹ã€‚\n",
        "\n",
        "    Args:\n",
        "        file_path: .edf æ–‡ä»¶çš„è·¯å¾„\n",
        "        start_sec: èµ·å§‹æ—¶é—´ (ç§’)\n",
        "        end_sec: ç»“æŸæ—¶é—´ (ç§’)\n",
        "        target_channels: ç”¨æˆ·æŒ‡å®šçš„ç›®æ ‡é€šé“åˆ—è¡¨ã€‚\n",
        "\n",
        "    Returns:\n",
        "        (data, final_channels, sample_rate, removed_channels)\n",
        "        data: å½¢çŠ¶ä¸º (n_channels, n_samples) çš„numpyæ•°ç»„ï¼Œå¦‚æœæœªæ‰¾åˆ°ä»»ä½•ç›®æ ‡é€šé“åˆ™ä¸ºNone\n",
        "        final_channels: æœ€ç»ˆä¿ç•™çš„é€šé“åç§°åˆ—è¡¨ï¼ˆå·²å»é‡ï¼‰\n",
        "        sample_rate: é‡‡æ ·ç‡ (Hz)\n",
        "        removed_channels: è¢«ç§»é™¤çš„é‡å¤é€šé“åˆ—è¡¨\n",
        "    \"\"\"\n",
        "    with pyedflib.EdfReader(str(file_path)) as f:\n",
        "        sample_rate = f.getSampleFrequency(0)\n",
        "        all_file_channels = f.getSignalLabels()\n",
        "\n",
        "        # æ‰¾å‡ºæ–‡ä»¶ä¸­å­˜åœ¨ä¸”åœ¨ç›®æ ‡åˆ—è¡¨ä¸­çš„é€šé“\n",
        "        common_channels = [ch for ch in target_channels if ch in all_file_channels]\n",
        "\n",
        "        if not common_channels:\n",
        "            print(f\"  æ–‡ä»¶ {file_path.name} ä¸­æœªæ‰¾åˆ°ä»»ä½•ç›®æ ‡é€šé“ã€‚\")\n",
        "            return None, [], sample_rate, []\n",
        "\n",
        "        # å»é‡ï¼Œä¿æŒé¡ºåº\n",
        "        seen = set()\n",
        "        final_channels = []\n",
        "        removed_duplicates = []\n",
        "        for ch in common_channels:\n",
        "            if ch in seen:\n",
        "                removed_duplicates.append(ch)\n",
        "            else:\n",
        "                seen.add(ch)\n",
        "                final_channels.append(ch)\n",
        "\n",
        "        if removed_duplicates:\n",
        "            print(f\"  åœ¨æ–‡ä»¶ {file_path.name} ä¸­æ£€æµ‹åˆ°å¹¶ç§»é™¤äº†é‡å¤é€šé“: {removed_duplicates}\")\n",
        "\n",
        "        # è®¡ç®—æ ·æœ¬ç´¢å¼•\n",
        "        start_sample = int(start_sec * sample_rate)\n",
        "        end_sample = int(end_sec * sample_rate)\n",
        "\n",
        "        # è¯»å–æ•°æ®\n",
        "        data = np.vstack([\n",
        "            f.readSignal(all_file_channels.index(ch), start_sample, end_sample - start_sample)\n",
        "            for ch in final_channels\n",
        "        ])\n",
        "\n",
        "        return data, final_channels, sample_rate, removed_duplicates\n",
        "\n",
        "\n",
        "def process_segments(segment_list: List, data_label: str):\n",
        "    \"\"\"\n",
        "    å¤„ç† preictal æˆ– interictal æ®µã€‚\n",
        "    æ”¯æŒä¸¤ç§ç»“æ„ï¼š\n",
        "        - preictal: [[('f1',(s,e)), ('f2',(s,e))], ...] â†’ æ¯ä¸ªå…ƒç»„æ˜¯ä¸€æ®µ\n",
        "        - interictal: [('f1',(s,e)), ('f2',(s,e)), ...] â†’ æ¯ä¸ªå…ƒç»„æ˜¯ä¸€æ®µ\n",
        "\n",
        "    æ‰€æœ‰æ®µç‹¬ç«‹å¤„ç†ï¼Œä¸åˆå¹¶ã€‚\n",
        "\n",
        "    Returns:\n",
        "        all_data: æ¯ä¸ªæœ‰æ•ˆæ®µéƒ½ä½œä¸ºä¸€ä¸ªç‹¬ç«‹é¡¹åŠ å…¥ç»“æœåˆ—è¡¨\n",
        "    \"\"\"\n",
        "    all_data = []\n",
        "    global_segment_index = 0  # ç”¨äºå…¨å±€ç¼–å·\n",
        "\n",
        "    for i, segment in enumerate(segment_list):\n",
        "        # ç»Ÿä¸€å±•å¼€æˆ file-time å¯¹åˆ—è¡¨\n",
        "        if isinstance(segment, list):\n",
        "            if len(segment) == 0:\n",
        "                continue\n",
        "            if isinstance(segment[0], tuple) and isinstance(segment[0][1], tuple):\n",
        "                # æƒ…å†µ1: segment æ˜¯åˆ—è¡¨ï¼Œå¦‚ [('f1', (s,e)), ('f2', (s,e))]\n",
        "                file_time_pairs = segment\n",
        "            else:\n",
        "                continue\n",
        "        elif isinstance(segment, tuple) and isinstance(segment[1], tuple):\n",
        "            # æƒ…å†µ2: segment æ˜¯å•ä¸ªå…ƒç»„ï¼Œå¦‚ ('f1', (s,e))\n",
        "            file_time_pairs = [segment]\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        # ç°åœ¨ file_time_pairs æ˜¯ [(), (), ...]ï¼Œæ¯ä¸ªå…ƒç»„æ˜¯ä¸€æ®µ\n",
        "        for file_name, (start_sec, end_sec) in file_time_pairs:\n",
        "            global_segment_index += 1\n",
        "            print(f\"æ­£åœ¨å¤„ç†ç¬¬ {global_segment_index} ä¸ª {data_label} æ®µ: {file_name} ({start_sec:.1f} - {end_sec:.1f}s)\")\n",
        "\n",
        "            file_path = DATA_ROOT / file_name\n",
        "            if not file_path.exists():\n",
        "                print(f\"  è­¦å‘Š: æ–‡ä»¶ {file_path} ä¸å­˜åœ¨ï¼Œè·³è¿‡...\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                data_segment, channels, sample_rate, removed = read_edf_segment(\n",
        "                    file_path, start_sec, end_sec, TARGET_CHANNELS\n",
        "                )\n",
        "\n",
        "                if data_segment is None:\n",
        "                    print(f\"  æ–‡ä»¶ {file_name} ä¸­æ— æœ‰æ•ˆç›®æ ‡é€šé“ï¼Œè·³è¿‡æ­¤æ—¶é—´æ®µã€‚\")\n",
        "                    continue\n",
        "\n",
        "                duration = data_segment.shape[1] / sample_rate\n",
        "\n",
        "                print(f\"  è¯»å–æˆåŠŸ: å½¢çŠ¶ {data_segment.shape}, æ—¶é•¿ {duration:.1f}s, \"\n",
        "                      f\"ä¿ç•™ {len(channels)} ä¸ªé€šé“\")\n",
        "                if removed:\n",
        "                    print(f\"  ç§»é™¤é‡å¤é€šé“: {list(set(removed))}\")\n",
        "\n",
        "                all_data.append({\n",
        "                    'data': data_segment,\n",
        "                    'channels': channels,\n",
        "                    'sample_rate': sample_rate,\n",
        "                    'duration_seconds': duration,\n",
        "                    'file': file_name,\n",
        "                    'start_sec': start_sec,\n",
        "                    'end_sec': end_sec\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  è¯»å– {file_name} æ—¶å‡ºé”™: {e}\")\n",
        "                continue\n",
        "\n",
        "            print(\"-\" * 60)\n",
        "\n",
        "    print(f\"\\nâœ… æ€»å…±æˆåŠŸæå–äº† {len(all_data)} ä¸ªç‹¬ç«‹çš„ {data_label} æ®µã€‚\\n\")\n",
        "    return all_data\n",
        "\n",
        "\n",
        "# ========================\n",
        "# ä¸»å¤„ç†æµç¨‹\n",
        "# ========================\n",
        "\n",
        "# å¤„ç† preictal æ•°æ®ï¼ˆå®Œå…¨ä¿ç•™åŸé€»è¾‘ï¼‰\n",
        "print(\"ğŸš€ å¼€å§‹å¤„ç† preictal (é¢„å‘ä½œæœŸ) æ•°æ®...\\n\")\n",
        "all_preictal_data = process_segments(preictal_segments, \"preictal\")\n",
        "\n",
        "# å¤„ç† interictal æ•°æ®ï¼ˆæ–°å¢ï¼‰\n",
        "print(\"ğŸš€ å¼€å§‹å¤„ç† interictal (é—´æ­‡æœŸ) æ•°æ®...\\n\")\n",
        "all_interictal_data = process_segments(interictal_segments, \"interictal\")\n",
        "\n",
        "# æœ€ç»ˆç»“æœ\n",
        "print(\"=\" * 60)\n",
        "print(\"âœ… æ‰€æœ‰æ•°æ®å¤„ç†å®Œæˆï¼\")\n",
        "print(f\"æˆåŠŸæå– preictal æ®µæ•°: {len(all_preictal_data)}\")\n",
        "print(f\"æˆåŠŸæå– interictal æ®µæ•°: {len(all_interictal_data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "é€šè¿‡å…¬å¼è®¡ç®—preictalçš„æ»‘åŠ¨çª—å£æ­¥é•¿ï¼Œè¿›è¡Œæ ·æœ¬åˆ†å‰²ï¼ˆ5sï¼‰ï¼Œä½¿å¾—preictalæ•°æ®é‡å’Œinterictalç›¸ä¼¼ï¼Œè§£å†³æ•°æ®ä¸å¹³è¡¡é—®é¢˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "preictalæ–‡ä»¶ chb23_06.edf å½¢çŠ¶ï¼š(719, 22, 1280)\n",
            "preictalæ–‡ä»¶ chb23_07.edf å½¢çŠ¶ï¼š(657, 22, 1280)\n",
            "preictalæ–‡ä»¶ chb23_08.edf å½¢çŠ¶ï¼š(9, 22, 1280)\n",
            "preictalæ–‡ä»¶ chb23_08.edf å½¢çŠ¶ï¼š(719, 22, 1280)\n",
            "preictalæ–‡ä»¶ chb23_09.edf å½¢çŠ¶ï¼š(719, 22, 1280)\n",
            "preictalæ–‡ä»¶ chb23_09.edf å½¢çŠ¶ï¼š(719, 22, 1280)\n",
            "æ‰€æœ‰ preictal æ®µæ€»æ ·æœ¬æ•°: 3542\n",
            "interictalæ–‡ä»¶ chb23_10.edf å½¢çŠ¶ï¼š(957, 22, 1280)\n",
            "interictalæ–‡ä»¶ chb23_16.edf å½¢çŠ¶ï¼š(2880, 22, 1280)\n",
            "interictalæ–‡ä»¶ chb23_17.edf å½¢çŠ¶ï¼š(2517, 22, 1280)\n",
            "interictalæ–‡ä»¶ chb23_19.edf å½¢çŠ¶ï¼š(2880, 22, 1280)\n",
            "interictalæ–‡ä»¶ chb23_20.edf å½¢çŠ¶ï¼š(1001, 22, 1280)\n",
            "æ‰€æœ‰ interictal æ®µæ€»æ ·æœ¬æ•°: 10235\n",
            "Patient 23 æ•°æ®å·²ä¿å­˜ä¸º HDF5 æ ¼å¼ï¼š\n",
            "  Preictal: D:\\é™ˆæ•™æˆç»„\\mymodel\\data\\unprocessed\\preictal\\preictal_fragments23.h5\n",
            "  Interictal: D:\\é™ˆæ•™æˆç»„\\mymodel\\data\\unprocessed\\interictal\\interictal_fragments23.h5\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# è®¾ç½®é‡‡æ ·çª—å£å’Œé€šé“æ•°\n",
        "S = 5  # ç§’\n",
        "CHANNEL_NUM = 22  # é€šé“æ•°\n",
        "\n",
        "# ç»Ÿè®¡ preictal å’Œ interictal çš„æ€»æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œåˆå¹¶æ‰€æœ‰æ®µ\n",
        "M = sum([d['duration_seconds'] for d in all_preictal_data])\n",
        "N = sum([d['duration_seconds'] for d in all_interictal_data])\n",
        "\n",
        "# è®¡ç®—æ¯”ä¾‹ K\n",
        "K = M / N if N > 0 else 1\n",
        "'''\n",
        "print(f\"æ‰€æœ‰ preictal æ®µæ€»æ—¶é•¿ M: {M:.2f} ç§’\")\n",
        "print(f\"æ‰€æœ‰ interictal æ®µæ€»æ—¶é•¿ N: {N:.2f} ç§’\")\n",
        "print(f\"çª—å£é•¿åº¦ S: {S} ç§’\")\n",
        "print(f\"æ­¥é•¿æ¯”ä¾‹ K: {K:.4f}\")\n",
        "'''\n",
        "K_DETERMINED = 0.5  # å¼ºåˆ¶è®¾ç½® K=0.5\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨\n",
        "save_dir = f\"D:\\\\é™ˆæ•™æˆç»„\\\\mymodel\\\\data\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# æ„å»ºä¿å­˜è·¯å¾„ï¼ˆä½¿ç”¨ .h5 æ‰©å±•åï¼‰\n",
        "save_path_preictal = f\"{save_dir}\\\\unprocessed\\\\preictal\\\\preictal_fragments{PATIENT_NUM:02d}.h5\"\n",
        "save_path_interictal = f\"{save_dir}\\\\unprocessed\\\\interictal\\\\interictal_fragments{PATIENT_NUM:02d}.h5\"\n",
        "\n",
        "# åˆ†å‰²å¹¶ä¿å­˜ preictal ç‰‡æ®µ\n",
        "with h5py.File(save_path_preictal, 'w') as f:\n",
        "    total_preictal_sample = 0\n",
        "    for seg_idx, seg in enumerate(all_preictal_data, 1):\n",
        "        preictal_list = []\n",
        "        data = seg['data']  # shape: (channels, timepoints)\n",
        "        sample_rate = seg['sample_rate']\n",
        "        total_points = data.shape[1]\n",
        "        window_points = int(S * sample_rate)\n",
        "        step_points = int(S * K_DETERMINED * sample_rate)\n",
        "        # åªä¿ç•™å‰22é€šé“\n",
        "        data = data[:CHANNEL_NUM, :]\n",
        "        # è®¡ç®—ç‰‡æ®µæ•°\n",
        "        a = int(np.floor((data.shape[1] - window_points) / step_points) + 1)\n",
        "        total_preictal_sample += a\n",
        "        for i in range(a):\n",
        "            start = i * step_points\n",
        "            end = start + window_points\n",
        "            if end > total_points:\n",
        "                break\n",
        "            frag = data[:, start:end]\n",
        "            preictal_list.append(frag)\n",
        "        preictal_fragments = np.stack(preictal_list, axis = 0)\n",
        "        f.create_dataset(f'fragment_{seg_idx:02d}', data=preictal_fragments, compression='gzip')\n",
        "        print(f\"preictalæ–‡ä»¶ {seg['file']} å½¢çŠ¶ï¼š{preictal_fragments.shape}\")\n",
        "    print(f\"æ‰€æœ‰ preictal æ®µæ€»æ ·æœ¬æ•°: {total_preictal_sample}\")\n",
        "\n",
        "# åˆ†å‰² interictal æ®µ\n",
        "with h5py.File(save_path_interictal, 'w') as f:\n",
        "    total_interictal_sample = 0\n",
        "    for seg_idx, seg in enumerate(all_interictal_data, 1):\n",
        "        interictal_list = []\n",
        "        data = seg['data']\n",
        "        sample_rate = seg['sample_rate']\n",
        "        total_points = data.shape[1]\n",
        "        window_points = int(S * sample_rate)\n",
        "        step_points = int(S * sample_rate)\n",
        "        data = data[:CHANNEL_NUM, :]\n",
        "        b = int(np.floor((data.shape[1] - window_points) / step_points) + 1)\n",
        "        total_interictal_sample += b\n",
        "        for i in range(b):\n",
        "            start = i * step_points\n",
        "            end = start + window_points\n",
        "            if end > total_points:\n",
        "                break\n",
        "            frag = data[:, start:end]\n",
        "            interictal_list.append(frag)\n",
        "        interictal_fragments = np.stack(interictal_list, axis = 0)\n",
        "        f.create_dataset(f'fragment_{seg_idx:02d}', data=interictal_fragments, compression='gzip')\n",
        "        print(f\"interictalæ–‡ä»¶ {seg['file']} å½¢çŠ¶ï¼š{interictal_fragments.shape}\")\n",
        "    print(f\"æ‰€æœ‰ interictal æ®µæ€»æ ·æœ¬æ•°: {total_interictal_sample}\")\n",
        "\n",
        "print(f\"Patient {PATIENT_NUM:02d} æ•°æ®å·²ä¿å­˜ä¸º HDF5 æ ¼å¼ï¼š\")\n",
        "print(f\"  Preictal: {save_path_preictal}\")\n",
        "print(f\"  Interictal: {save_path_interictal}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "series_list = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "data = pd.Series({\n",
        "    'Patient_Number': PATIENT_NUM,\n",
        "    'Preictal_Time_Duration': round(M,2) ,\n",
        "    'Interictal_Time_Duration': round(N,2),\n",
        "    'Theoretical_Step_Length_Ratio': round(K,4),\n",
        "    'Determined_Step_Length_Ratio': K_DETERMINED,\n",
        "    'Total_Preictal_Samples': total_preictal_sample,\n",
        "    'Total_Interictal_Samples': total_interictal_sample\n",
        "})\n",
        "series_list.append(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "æ‚£è€…æ•°æ®ç»Ÿè®¡è¡¨ï¼š\n",
            "====================================================================================================\n",
            " Patient_Number  Preictal_Time_Duration  Interictal_Time_Duration  Theoretical_Step_Length_Ratio  Determined_Step_Length_Ratio  Total_Preictal_Samples  Total_Interictal_Samples\n",
            "         1.0000              10528.0000                51743.0000                         0.2035                        0.5000               4197.0000                10347.0000\n",
            "         2.0000               5400.0000                93759.0000                         0.0576                        0.5000               2157.0000                18751.0000\n",
            "         3.0000              10358.0000                98620.0000                         0.1050                        0.5000               4131.0000                19723.0000\n",
            "         4.0000               3179.0000               422020.0000                         0.0075                        0.5000               1269.0000                84398.0000\n",
            "         5.0000               8987.0000                52019.0000                         0.1728                        0.5000               3585.0000                10403.0000\n",
            "         6.0000              17605.0000                89269.0000                         0.1972                        0.5000               7027.0000                17851.0000\n",
            "         7.0000               5400.0000               184756.0000                         0.0292                        0.5000               2157.0000                36949.0000\n",
            "         8.0000               8992.0000                 3623.0000                         2.4819                        0.5000               3590.0000                  724.0000\n",
            "         9.0000               7200.0000               166899.0000                         0.0431                        0.5000               2876.0000                33376.0000\n",
            "        10.0000              11883.0000                87395.0000                         0.1360                        0.5000               4746.0000                17478.0000\n",
            "        11.0000               2954.0000               111600.0000                         0.0265                        0.5000               1179.0000                22320.0000\n",
            "        14.0000              10335.0000                16967.0000                         0.6091                        0.5000               4123.0000                 3393.0000\n",
            "        16.0000               8979.0000                20318.0000                         0.4419                        0.5000               3581.0000                 4063.0000\n",
            "        17.0000               5400.0000                42494.0000                         0.1271                        0.5000               2157.0000                 8497.0000\n",
            "        18.0000               7158.0000                91570.0000                         0.0782                        0.5000               2857.0000                18314.0000\n",
            "        19.0000               3600.0000                93600.0000                         0.0385                        0.5000               1438.0000                18720.0000\n",
            "        20.0000               8279.0000                68651.0000                         0.1206                        0.5000               3303.0000                13728.0000\n",
            "        21.0000               7096.0000                84365.0000                         0.0841                        0.5000               2833.0000                16872.0000\n",
            "        22.0000               4563.0000                61419.0000                         0.0743                        0.5000               1822.0000                12282.0000\n",
            "        23.0000               8871.0000                51185.0000                         0.1733                        0.5000               3542.0000                10235.0000\n",
            "====================================================================================================\n",
            "\n",
            "è¡¨æ ¼å·²ä¿å­˜è‡³: D:\\é™ˆæ•™æˆç»„\\mymodel\\data\\patient_statistics.csv\n",
            "\n",
            "æ€»è®¡å¤„ç†æ‚£è€…æ•°: 20\n",
            "æ€»Preictalæ ·æœ¬æ•°: 62570\n",
            "æ€»Interictalæ ·æœ¬æ•°: 378424\n"
          ]
        }
      ],
      "source": [
        "# å°†æ‰€æœ‰ Series è½¬æ¢ä¸º DataFrame è¡¨æ ¼\n",
        "data_table = pd.concat(series_list, axis=1).T\n",
        "data_table.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# æ˜¾ç¤ºè¡¨æ ¼\n",
        "print(\"æ‚£è€…æ•°æ®ç»Ÿè®¡è¡¨ï¼š\")\n",
        "print(\"=\" * 100)\n",
        "print(data_table.to_string(index=False, float_format='%.4f'))\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# ä¹Ÿå¯ä»¥ä¿å­˜ä¸º CSV æ–‡ä»¶\n",
        "csv_path = f\"D:\\\\é™ˆæ•™æˆç»„\\\\mymodel\\\\data\\\\patient_statistics.csv\"\n",
        "data_table.to_csv(csv_path, index=False, float_format='%.4f')\n",
        "print(f\"\\nè¡¨æ ¼å·²ä¿å­˜è‡³: {csv_path}\")\n",
        "\n",
        "# æ˜¾ç¤ºä¸€äº›ç»Ÿè®¡ä¿¡æ¯\n",
        "print(f\"\\næ€»è®¡å¤„ç†æ‚£è€…æ•°: {len(data_table)}\")\n",
        "print(f\"æ€»Preictalæ ·æœ¬æ•°: {data_table['Total_Preictal_Samples'].sum():.0f}\")\n",
        "print(f\"æ€»Interictalæ ·æœ¬æ•°: {data_table['Total_Interictal_Samples'].sum():.0f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
