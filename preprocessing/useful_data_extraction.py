import h5py
import json
import pandas as pd
import pyedflib
import numpy as np
from pathlib import Path
import os
from typing import List, Tuple, Optional

DATA_ROOT = Path(r'D:\é™ˆæ•™æˆç»„\CHB-MIT') 
SAVE_DIR = r"D:\é™ˆæ•™æˆç»„\mymodel\data2"
USEFUL_DATA_INFO_ROOT = Path(r'D:\é™ˆæ•™æˆç»„\mymodel\data\useful_data_info.json')
PATIENT_NUM = [i for i in range(1, 25) if i not in [12, 13, 15, 24]]  # å‡è®¾æœ‰24ä¸ªæ‚£è€…
SAMPLE_LENGTH = 5  # ç§’
CHANNEL_NUM = 22  # é€šé“æ•°
SAMPLING_OVERLAP = 1  # é‡å æ¯”ä¾‹
TARGET_CHANNELS = [
    'C3-P3', 'C4-P4', 'CZ-PZ', 'F3-C3', 'F4-C4', 'F7-T7', 'F8-T8', 'FP1-F3', 'FP1-F7', 'FP2-F4',
    'FP2-F8', 'FT10-T8', 'FT9-FT10', 'FZ-CZ', 'P3-O1', 'P4-O2', 'P7-O1', 'P7-T7', 'P8-O2', 'T7-FT9',
    'T7-P7', 'T8-P8'
]

def read_edf_segment(
    file_path: Path,
    start_sec: float,
    end_sec: float,
    target_channels: List[str]
) -> Tuple[Optional[np.ndarray], List[str], float, List[str]]:
  
    with pyedflib.EdfReader(str(file_path)) as f:
        sample_rate = f.getSampleFrequency(0)
        all_file_channels = f.getSignalLabels()

        # æ‰¾å‡ºæ–‡ä»¶ä¸­å­˜åœ¨ä¸”åœ¨ç›®æ ‡åˆ—è¡¨ä¸­çš„é€šé“
        common_channels = [ch for ch in target_channels if ch in all_file_channels]

        if not common_channels:
            print(f"  æ–‡ä»¶ {file_path.name} ä¸­æœªæ‰¾åˆ°ä»»ä½•ç›®æ ‡é€šé“ã€‚")
            return None, [], sample_rate, []

        # å»é‡ï¼Œä¿æŒé¡ºåº
        seen = set()
        final_channels = []
        removed_duplicates = []
        for ch in common_channels:
            if ch in seen:
                removed_duplicates.append(ch)
            else:
                seen.add(ch)
                final_channels.append(ch)

        if removed_duplicates:
            print(f"  åœ¨æ–‡ä»¶ {file_path.name} ä¸­æ£€æµ‹åˆ°å¹¶ç§»é™¤äº†é‡å¤é€šé“: {removed_duplicates}")

        # è®¡ç®—æ ·æœ¬ç´¢å¼•
        start_sample = int(start_sec * sample_rate)
        end_sample = int(end_sec * sample_rate)

        # è¯»å–æ•°æ®
        data = np.vstack([
            f.readSignal(all_file_channels.index(ch), start_sample, end_sample - start_sample)
            for ch in final_channels
        ])

        return data, final_channels, sample_rate, removed_duplicates


def process_segments(segment_list: List, data_label: str, parent_file_path: Path) -> List[dict]:

    all_data = []
    global_segment_index = 0  # ç”¨äºå…¨å±€ç¼–å·

    for i, segment in enumerate(segment_list, 1):

        file_name, time_interval = segment
        global_segment_index += 1
        start_sec, end_sec = time_interval
        print(f"æ­£åœ¨å¤„ç†ç¬¬ {global_segment_index} ä¸ª {data_label} æ®µ: {file_name} ({start_sec:.1f} - {end_sec:.1f}s)")
        file_path = Path(parent_file_path) / file_name
        if not file_path.exists():
            print(f"  è­¦å‘Š: æ–‡ä»¶ {file_path} ä¸å­˜åœ¨ï¼Œè·³è¿‡...")
            continue

        try:
            data_segment, channels, sample_rate, removed = read_edf_segment(
                file_path, start_sec, end_sec, TARGET_CHANNELS
            )

            if data_segment is None:
                print(f"  æ–‡ä»¶ {file_name} ä¸­æ— æœ‰æ•ˆç›®æ ‡é€šé“ï¼Œè·³è¿‡æ­¤æ—¶é—´æ®µã€‚")
                continue

            duration = data_segment.shape[1] / sample_rate

            print(f"  è¯»å–æˆåŠŸ: å½¢çŠ¶ {data_segment.shape}, æ—¶é•¿ {duration:.1f}s, "
                      f"ä¿ç•™ {len(channels)} ä¸ªé€šé“")
            if removed:
                print(f"  ç§»é™¤é‡å¤é€šé“: {list(set(removed))}")

            all_data.append({
                'data': data_segment,
                'channels': channels,
                'sample_rate': sample_rate,
                'duration_seconds': duration,
                'file': file_name,
                'start_sec': start_sec,
                'end_sec': end_sec
            })

        except Exception as e:
            print(f"  è¯»å– {file_name} æ—¶å‡ºé”™: {e}")
            continue

        print("-" * 60)

    print(f"\nâœ… æ€»å…±æˆåŠŸæå–äº† {len(all_data)} ä¸ªç‹¬ç«‹çš„ {data_label} æ®µã€‚\n")
    return all_data


if __name__ == "__main__":

    patient_data_statistics_list = []  # ç”¨äºå­˜å‚¨æ‰€æœ‰æ‚£è€…çš„æ•°æ®ç»Ÿè®¡ä¿¡æ¯
    
    with open(USEFUL_DATA_INFO_ROOT, 'r', encoding='utf-8') as f:
        useful_data_info = json.load(f)

    for i, patient_data in enumerate(useful_data_info):
        preictal_segments = patient_data[0]
        interictal_segments = patient_data[1]
        patient = PATIENT_NUM[i]
        print("ğŸš€ å¼€å§‹å¤„ç† preictal (é¢„å‘ä½œæœŸ) æ•°æ®...\n")
        all_preictal_data = process_segments(preictal_segments, "preictal", parent_file_path = DATA_ROOT / f"chb{patient:02d}")
       
        # å¤„ç† interictal æ•°æ®
        print("ğŸš€ å¼€å§‹å¤„ç† interictal (é—´æ­‡æœŸ) æ•°æ®...\n")
        all_interictal_data = process_segments(interictal_segments, "interictal", parent_file_path = DATA_ROOT / f"chb{patient:02d}")
        
        # ç»Ÿè®¡ preictal å’Œ interictal çš„æ€»æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œåˆå¹¶æ‰€æœ‰æ®µ
        total_preictal_duration = sum([d['duration_seconds'] for d in all_preictal_data])
        total_interictal_duration = sum([d['duration_seconds'] for d in all_interictal_data])

        # æ–°å¢ï¼šæ„å»ºå­ç›®å½•è·¯å¾„
        preictal_subdir = os.path.join(SAVE_DIR, "preictal")
        interictal_subdir = os.path.join(SAVE_DIR, "interictal")

        # ç¡®ä¿æ‰€æœ‰éœ€è¦çš„ç›®å½•éƒ½å­˜åœ¨ï¼ˆè‡ªåŠ¨åˆ›å»ºå¤šçº§ç›®å½•ï¼‰
        os.makedirs(preictal_subdir, exist_ok=True)
        os.makedirs(interictal_subdir, exist_ok=True)

        # æ„å»ºä¿å­˜è·¯å¾„
        save_path_preictal = os.path.join(preictal_subdir, f"preictal_fragments{patient:02d}.h5")
        save_path_interictal = os.path.join(interictal_subdir, f"interictal_fragments{patient:02d}.h5")

        # åˆ†å‰²å¹¶ä¿å­˜ preictal ç‰‡æ®µ
        if total_preictal_duration < total_interictal_duration:
            preictal_step, interictal_step = SAMPLE_LENGTH * SAMPLING_OVERLAP, SAMPLE_LENGTH
        else:
            preictal_step, interictal_step = SAMPLE_LENGTH, SAMPLE_LENGTH * SAMPLING_OVERLAP
            print(f"âš ï¸ Patient {patient:02d} çš„ interictal æ€»æ—¶é•¿ä¸å¤Ÿï¼Œè°ƒæ•´é‡‡æ ·æ­¥é•¿ä¸º {interictal_step}s")

        with h5py.File(save_path_preictal, 'w') as f:
            total_preictal_sample = 0
            for seg_idx, seg in enumerate(all_preictal_data, 1):
                preictal_list = []
                data = seg['data']  # shape: (channels, timepoints)
                sample_rate = seg['sample_rate']
                total_points = data.shape[1]
                window_points = int(SAMPLE_LENGTH * sample_rate)
                step_points = int(preictal_step * sample_rate)
                # åªä¿ç•™å‰22é€šé“
                data = data[:CHANNEL_NUM, :]
                # è®¡ç®—ç‰‡æ®µæ•°
                a = int(np.floor((data.shape[1] - window_points) / step_points) + 1)
                total_preictal_sample += a
                for i in range(a):
                    start = i * step_points
                    end = start + window_points
                    if end > total_points:
                        break
                    frag = data[:, start:end]
                    preictal_list.append(frag)
                if preictal_list:  # é¿å…ç©ºåˆ—è¡¨å †å 
                    preictal_fragments = np.stack(preictal_list, axis=0)
                    f.create_dataset(f'fragment_{seg_idx:02d}', data=preictal_fragments, compression='gzip')
                    print(f"preictalæ–‡ä»¶ {seg['file']} å½¢çŠ¶ï¼š{preictal_fragments.shape}")
                else:
                    print(f"preictalæ–‡ä»¶ {seg['file']} æœªç”Ÿæˆä»»ä½•ç‰‡æ®µ")
            print(f"æ‰€æœ‰ preictal æ®µæ€»æ ·æœ¬æ•°: {total_preictal_sample}")

        # åˆ†å‰²å¹¶ä¿å­˜ interictal ç‰‡æ®µ
        with h5py.File(save_path_interictal, 'w') as f:
            total_interictal_sample = 0
            for seg_idx, seg in enumerate(all_interictal_data, 1):
                interictal_list = []
                data = seg['data']
                sample_rate = seg['sample_rate']
                total_points = data.shape[1]
                window_points = int(SAMPLE_LENGTH * sample_rate)
                step_points = int(interictal_step * sample_rate)  # æ³¨æ„ï¼šè¿™é‡Œ K_DETERMINED æœªä½¿ç”¨
                data = data[:CHANNEL_NUM, :]
                b = int(np.floor((data.shape[1] - window_points) / step_points) + 1)
                total_interictal_sample += b
                for i in range(b):
                    start = i * step_points
                    end = start + window_points
                    if end > total_points:
                        break
                    frag = data[:, start:end]
                    interictal_list.append(frag)
                if interictal_list:  # é¿å…ç©ºåˆ—è¡¨å †å 
                    interictal_fragments = np.stack(interictal_list, axis=0)
                    f.create_dataset(f'fragment_{seg_idx:02d}', data=interictal_fragments, compression='gzip')
                    print(f"interictalæ–‡ä»¶ {seg['file']} å½¢çŠ¶ï¼š{interictal_fragments.shape}")
                else:
                    print(f"interictalæ–‡ä»¶ {seg['file']} æœªç”Ÿæˆä»»ä½•ç‰‡æ®µ")
                if total_interictal_sample >= total_preictal_sample:
                    print("å·²è¾¾åˆ°ä¸ preictal ç›¸åŒçš„æ ·æœ¬æ•°ï¼Œåœæ­¢å¤„ç†æ›´å¤š interictal æ®µã€‚")
                    break
            print(f"æ‰€æœ‰ interictal æ®µæ€»æ ·æœ¬æ•°: {total_interictal_sample}")
            
        print(f"Patient {patient:02d} æ•°æ®å·²ä¿å­˜ä¸º HDF5 æ ¼å¼ï¼š")
        patient_data_statistics = pd.Series({
            'Patient_Number': patient,
            'Preictal_Time_Duration': round(total_preictal_duration,2) ,
            'Interictal_Time_Duration': round(total_interictal_duration,2),
            'Total_Preictal_Samples_Selected': total_preictal_sample,
            'Total_Interictal_Samples_Selected': total_interictal_sample
        })
        patient_data_statistics_list.append(patient_data_statistics)

    # å°†æ‰€æœ‰ Series è½¬æ¢ä¸º DataFrame è¡¨æ ¼
    data_table = pd.concat(patient_data_statistics_list, axis=1).T
    data_table.reset_index(drop=True, inplace=True)

    # ä¿å­˜ä¸º CSV æ–‡ä»¶
    csv_save_path = os.path.join(SAVE_DIR, "patient_data_statistics.csv")
    data_table.to_csv(csv_save_path, index=False, float_format='%.4f')
    print(f"æ‰€æœ‰æ‚£è€…çš„æ•°æ®ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜ä¸º CSV æ–‡ä»¶ï¼š{csv_save_path}")
