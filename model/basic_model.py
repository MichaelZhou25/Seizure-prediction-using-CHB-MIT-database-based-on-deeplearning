import h5py
import numpy as np
import torch
from scipy.signal import butter, filtfilt, iirnotch
import matplotlib.pyplot as plt
from pathlib import Path
import os
from tqdm import tqdm  # è¿›åº¦æ¡
import warnings
from sklearn.model_selection import StratifiedKFold
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import accuracy_score
import time  


# å¿½ç•¥å¸¸è§è­¦å‘Š
warnings.filterwarnings("ignore")

# è®¾ç½®éšæœºç§å­
np.random.seed(42)
torch.manual_seed(42)

# -------------------------------
# é…ç½®å‚æ•°
# -------------------------------
PATIENT_ID = 23
DATA_DIR = Path("D:\\é™ˆæ•™æˆç»„\\mymodel\\data")
PREICTAL_PATH = DATA_DIR / "preictal" / f"preictal_fragments{PATIENT_ID:02d}.h5"
INTERICTAL_PATH = DATA_DIR / "interictal" / f"interictal_fragments{PATIENT_ID:02d}.h5"

# ä¿¡å·å‚æ•°
FS = 256  # é‡‡æ ·ç‡ (Hz)
N_FFT = 256
HOP_LENGTH = 128
WIN_LENGTH = 256

# è®¾å¤‡é€‰æ‹©ï¼ˆGPU åŠ é€Ÿï¼‰
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"ä½¿ç”¨è®¾å¤‡: {DEVICE}")



#ä¸€ã€æ•°æ®é¢„å¤„ç†éƒ¨åˆ†

# -------------------------------
# æ»¤æ³¢å‡½æ•°
# -------------------------------
def bandpass_filter(data, lowcut, highcut, fs, axis=-1, order=4):

    nyq = 0.5 * fs
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='band')
    return filtfilt(b, a, data, axis=axis)

def notch_filter(data, freq, fs, Q=30, axis=-1):

    b, a = iirnotch(freq, Q, fs)
    return filtfilt(b, a, data, axis=axis)

def remove_dc_component(data, axis=-1):

    mean_val = np.mean(data, axis=axis, keepdims=True)
    return data - mean_val

def apply_all_filters(data, fs=FS):

    filtered = notch_filter(data, 60.0, fs=fs)
    filtered = notch_filter(filtered, 120.0, fs=fs)
    filtered = remove_dc_component(filtered)
    return filtered


def generating_raw_data_matching_stft(data_array, n_fft=N_FFT, hop_length=HOP_LENGTH, device=DEVICE):
    if data_array.size == 0:
        return np.array([])

    # è½¬ä¸º tensor å¹¶ç§»åˆ°è®¾å¤‡
    data_tensor = torch.tensor(data_array, dtype=torch.float32).to(device)  # (N, C, timepoints)

    return data_tensor.unfold(-1, n_fft, hop_length).permute(0, 2, 1, 3)  # (N, C, T, t) -> (N, T, C, t)


# -------------------------------
# STFT å˜æ¢å‡½æ•°ï¼ˆæ”¯æŒæ‰¹é‡ + GPUï¼‰
# -------------------------------
def apply_stft_to_data(data_array, n_fft=N_FFT, hop_length=HOP_LENGTH,
                       win_length=WIN_LENGTH, fs=FS, device=DEVICE):

    if data_array.size == 0:
        return np.array([])

    n_samples, n_channels, n_timepoints = data_array.shape

    data_tensor = torch.tensor(data_array, dtype=torch.float32).to(device)  # (N, C, timepoints)

    window = torch.hann_window(win_length).to(device)

    data_flat = data_tensor.view(-1, n_timepoints)  # (N*C, timepoints)

    # åº”ç”¨ STFTï¼ˆè¿”å›å¤æ•°ï¼‰
    stft_complex = torch.stft(
        data_flat,
        n_fft=n_fft,
        hop_length=hop_length,
        win_length=win_length,
        window=window,
        center=True,          
        return_complex=True   
    )  # shape: (N*C, F0, T_frames)

    # æ¢å¤é€šé“ç»´åº¦
    freq_bins = n_fft // 2 + 1
    time_frames = stft_complex.shape[-1]
    stft_complex = stft_complex.view(n_samples, n_channels, freq_bins, time_frames)

    # åˆ é™¤ 0Hz (DC åˆ†é‡) å’Œ T_framesçš„é¦–å°¾å¸§
    stft_complex = stft_complex[:, :, 1:, 1:-1]  # (N, C, F, T)

    # è½¬ä¸ºå¯¹æ•°å¹…åº¦è°±ï¼š20 * log10(|Z| + Îµ)
    magnitude = stft_complex.abs()  # (N, C, F, T)
    log_magnitude_stft = 20 * torch.log10(magnitude + 1e-8)

    return log_magnitude_stft.permute(0, 3, 1, 2)  # (N, C, F, T) â†’ (N, T, C, F)

# -------------------------------
# ä¸»å¤„ç†å‡½æ•°ï¼šé€ç‰‡æ®µå¤„ç†å¹¶ä¿å­˜
# -------------------------------
def process_and_save_fragments(input_path, data_type):

    print(f"\næ­£åœ¨å¤„ç† {data_type} æ•°æ®...")

    input_list = []  # ç”¨äºæ”¶é›†æ‰€æœ‰æ¨¡å‹è¾“å…¥

    with h5py.File(input_path, 'r') as infile:
        keys = sorted(infile.keys())  # æŒ‰åç§°æ’åºï¼Œä¿è¯é¡ºåºä¸€è‡´

        for key in tqdm(keys, desc=f"{data_type.upper()} Processing", unit="frag"):
            raw_data = infile[key][()] 

            filtered_data = apply_all_filters(raw_data)

            stft_log_mag = apply_stft_to_data(filtered_data)  # (B, T, C, F)
            print(f"STFT å¯¹æ•°å¹…åº¦è°±å½¢çŠ¶: {stft_log_mag.shape}")
            raw_data = generating_raw_data_matching_stft(filtered_data) # (B, T, C, t)
            print(f"åŸå§‹æ—¶åŸŸæ•°æ®å½¢çŠ¶: {raw_data.shape}")
            model_input = torch.cat([stft_log_mag, raw_data], dim=-1)  # (B, T, C, F+t)

            model_input = model_input.cpu().numpy()  # è½¬å› CPU å’Œ NumPy

            input_list.append(model_input)

    concatenated_data = np.concatenate(input_list, axis=0)  # (Total_samples, T, C, F+t)
    print(f"{data_type}æ•°æ®è¾“å‡ºå½¢çŠ¶: {concatenated_data.shape}")

    return concatenated_data

# äºŒã€æ¨¡å‹éƒ¨åˆ†

# -------------------------------
# æ¨¡å‹å„ä¸ªæ¨¡å—
# -------------------------------
class FrequencyBranch(nn.Module):
    def __init__(self, in_freq=128, out_freq=64):
        super().__init__()
        # åœ¨ in_freq=128ï¼ˆé¢‘ç‡ç‚¹ï¼‰ä¸Šåšå±€éƒ¨å·ç§¯
        self.local_conv = nn.Sequential(
            nn.Conv1d(1, 16, 3, padding=1),
            nn.BatchNorm1d(16),
            nn.ReLU(),
            nn.Conv1d(16, 1, 3, padding=1),
            nn.BatchNorm1d(1),
            nn.ReLU()
        )
        # 1x1 å·ç§¯é™ç»´ï¼šin_freq â†’ out_freq
        self.reduce = nn.Conv1d(in_freq, out_freq, kernel_size=1)

    def forward(self, x):

        B, T, C, Freq = x.shape

        x = x.reshape(B * T * C, 1, Freq)  # (B*T*C, 1, 128)

        x = self.local_conv(x)  # (B*T*C, 1, 128)

        x = self.reduce(x.permute(0, 2, 1))  # (B*T*C, 128, 1) â†’ (B*T*C, 64, 1)

        x = x.permute(0, 2, 1)  # (B*T*C, 1, 64)

        x = x.reshape(B, T, C, 64)  # (B, T, C, 64)

        return x  # (B, T, C, 64)

class TCNBlock(nn.Module):
    """å•ä¸ª TCN æ®‹å·®å—"""
    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1, downsample=False):
        super().__init__()
        self.downsample = downsample
        self.total_padding = dilation * (kernel_size - 1)  # å…¨éƒ¨ç”¨äºå·¦å¡«å……

        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, dilation=dilation)
        self.bn1 = nn.BatchNorm1d(out_channels)
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, dilation=dilation)
        self.bn2 = nn.BatchNorm1d(out_channels)

        # æ®‹å·®è·¯å¾„è°ƒæ•´
        self.residual = nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels or downsample else nn.Identity()
        self.pool = nn.MaxPool1d(2, stride=2) if downsample else None

    def forward(self, x):
        
        residual = self.residual(x)  
        if self.pool:
            residual = self.pool(residual)

        out = F.pad(x, (self.total_padding, 0))
        out = self.conv1(out)
        out = self.bn1(out)
        out = self.relu(out)

        out = F.pad(out, (self.total_padding, 0))  
        out = self.conv2(out)
        out = self.bn2(out)

        if self.pool:
            out = self.pool(out)

        out += residual
        out = self.relu(out)
        return out

class TimeBranch(nn.Module):
    def __init__(self, in_time=256, out_time=64):
        super().__init__()
        
        self.tcn = nn.Sequential(
            TCNBlock(in_channels=1, out_channels=32, kernel_size=3, dilation=1),
            TCNBlock(in_channels=32, out_channels=out_time, kernel_size=3, dilation=2),
        )

    def forward(self, x):

        B, T, C, t = x.shape

        x = x.reshape(B * T * C, t, 1)  # (B*T*C, 256, 1)

        x = x.permute(0, 2, 1)  # â†’ (B*T*C, 1, 256)

        x = self.tcn(x)  # (B*T*C, 64, 256)

        x = x.mean(dim=-1)  # (B*T*C, 64)

        x = x.reshape(B, T, C, 64)  # (B, T, C, 64)

        return x  # (B, T, C, 64)

class AdjacencyMatrixLearning(nn.Module):
    def __init__(self, hidden_dim=64):
        super().__init__()

        self.hidden_dim = hidden_dim

        # æŠ•å½±ç½‘ç»œ
        self.W1 = nn.Linear(64, hidden_dim)  
        self.W2 = nn.Linear(64, hidden_dim)
        self.W3 = nn.Linear(64, hidden_dim) 
        self.W4 = nn.Linear(64, hidden_dim)

    def forward(self, freq_feature, time_feature):

        B, T, C, _ = freq_feature.shape

        # æŠ•å½±åˆ°ä½ç»´ç©ºé—´
        freq_feature = freq_feature.reshape(B * T, C, 64)
        time_feature = time_feature.reshape(B * T, C, 64)

        # Frequency Path
        f1 = torch.relu(self.W1(freq_feature))  # (B*T, C, hidden_dim)
        f2 = torch.relu(self.W2(freq_feature))
        A_freq = torch.bmm(f1, f2.transpose(1, 2)) # (B*T, C, C)

        # Time Path
        t1 = torch.relu(self.W3(time_feature))  # (B*T, C, hidden_dim)
        t2 = torch.relu(self.W4(time_feature))
        A_time = torch.bmm(t1, t2.transpose(1, 2)) # (B*T, C, C)

        # èåˆ + Softmax
        A = A_freq + A_time  # (B*T, C, C)
        A = A / (C ** 0.5)  # Scale
        A = torch.softmax(A, dim=-1)  # (B*T, C, C)

        A = A.reshape(B, T, C, C)  # (B, T, C, C)

        return A  
    
class TemporalGCN(nn.Module):
    def __init__(self, gcn_input_dim=64+64, gcn_layers=2, gru_hidden_dim=32):
        super().__init__()

        self.num_layers = gcn_layers
        self.gru_hidden_dim = gru_hidden_dim

        # å›¾å·ç§¯å±‚ï¼šä½¿ç”¨ Linear å®ç°ç‰¹å¾å˜æ¢
        self.graph_convs = nn.ModuleList()
        for i in range(gcn_layers):
            self.graph_convs.append(
                nn.Linear(gcn_input_dim, gcn_input_dim // 2)
            )
            gcn_input_dim = gcn_input_dim // 2  

        self.gcn_output_dim = gcn_input_dim

        # Bi-GRU å±‚
        self.gru = nn.GRU(
            input_size = self.gcn_output_dim,
            hidden_size = gru_hidden_dim,
            num_layers=1,
            bidirectional=True,
            batch_first=True
        )
        self.gru_output_dim = 2 * gru_hidden_dim  # bidirectional

    def forward(self, x, A):

        B, T, C, FT = x.shape

        x = x.reshape(B * T, C, FT)  # (B*T, C, 128)
        A = A.reshape(B * T, C, C)  # (B*T, C, C)

        # å¤šå±‚ GCN ä¼ æ’­
        for i in range(self.num_layers):
            # é‚»å±…èšåˆ: X' = A @ X
            x = torch.bmm(A, x)  # (B*T, C, 128)

            # ç‰¹å¾å˜æ¢: Linear layer
            x = self.graph_convs[i](x)  # (B*T, C, H)

            # éçº¿æ€§æ¿€æ´»
            x = F.relu(x)

        x = x.reshape(B, T, C, self.gcn_output_dim) # (B*T, C, 32) -> (B, T, C, 32)

        # åœ¨ç”µæé€šé“ç»´åº¦å–å¹³å‡: (B, T, C, 32) -> (B, T, 32)
        x_gcn = x.mean(dim=2)  

        x_tgcn, _ = self.gru(x_gcn)  # (B, T, 64)

        return x_tgcn  # (B, T, 64)

# -------------------------------
# æ¨¡å‹ä¸»ç»“æ„
# -------------------------------
class MainModel(nn.Module):
    def __init__(self, Freq=128, Time=256, num_classes=2):
        super().__init__()

        # ç‰¹å¾æå–
        self.freq_branch = FrequencyBranch(in_freq=Freq, out_freq=64)
        self.time_branch = TimeBranch(in_time=Time, out_time=64)

        # é‚»æ¥çŸ©é˜µå­¦ä¹ 
        self.adj_block = AdjacencyMatrixLearning(hidden_dim=32)

        # TGCN
        self.temporal_gcn = TemporalGCN(gcn_input_dim=64+64)

        # FC åˆ†ç±»
        self.classifier = nn.Sequential(
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(32, num_classes)
        )

    def forward(self, x):
        # x: (B, T=9, C=22, 384)
        freq_feat = x[:, :, :, :128]   # (B, 9, 22, 128)
        time_feat = x[:, :, :, 128:]   # (B, 9, 22, 256)

        freq_out = self.freq_branch(freq_feat)  # (B, 9, 22, 64)
        time_out = self.time_branch(time_feat)  # (B, 9, 22, 64)

        # æ‹¼æ¥èŠ‚ç‚¹ç‰¹å¾
        x_feat = torch.cat([freq_out, time_out], dim=3) # (B, 9, 22, 128)

        # å­¦ä¹ åŠ¨æ€é‚»æ¥çŸ©é˜µ
        A = self.adj_block(freq_out, time_out) # (B, 9, 22, 22)

        # TGCN
        x_tgcn = self.temporal_gcn(x_feat, A)  # (B, 9, 64)

        # æ—¶é—´æ³¨æ„åŠ›æ± åŒ–
        x_time_avg = x_tgcn.mean(dim=1)  # (B, 64)

        # åˆ†ç±»
        logits = self.classifier(x_time_avg) # (B, num_classes=2)

        return logits, A  
    
# ä¸‰ã€è®­ç»ƒå’Œè¯„ä¼°éƒ¨åˆ†
# -------------------------------
# è¶…å‚æ•°è®¾ç½®
# -------------------------------
BATCH_SIZE = 64
LEARNING_RATE = 0.001
NUM_EPOCHS = 10
PATIENCE = 10

# -------------------------------
# è®­ç»ƒå‡½æ•°
# -------------------------------
def plot_loss_and_accuracy(train_losses, val_losses, train_accuracies, val_accuracies):
    epochs = range(1, len(train_losses) + 1)

    plt.figure(figsize=(12, 5))

    # æŸå¤±æ›²çº¿
    plt.subplot(1, 2, 1)
    plt.plot(epochs, train_losses, 'b-', label='è®­ç»ƒæŸå¤±')
    plt.plot(epochs, val_losses, 'r-', label='éªŒè¯æŸå¤±')
    plt.title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±')
    plt.xlabel('è½®æ¬¡')
    plt.ylabel('æŸå¤±')
    plt.legend()

    # å‡†ç¡®ç‡æ›²çº¿
    plt.subplot(1, 2, 2)
    plt.plot(epochs, train_accuracies, 'b-', label='è®­ç»ƒå‡†ç¡®ç‡')
    plt.plot(epochs, val_accuracies, 'r-', label='éªŒè¯å‡†ç¡®ç‡')
    plt.title('è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡')
    plt.xlabel('è½®æ¬¡')
    plt.ylabel('å‡†ç¡®ç‡ (%)')
    plt.legend()

    plt.tight_layout()
    plt.show()

def train_model(model, train_loader, val_loader, num_epochs=NUM_EPOCHS, patience=PATIENCE):
    """
    è®­ç»ƒæ¨¡å‹å¹¶è¿”å›æœ€ä½³éªŒè¯å‡†ç¡®ç‡
    """
    model = model.to(DEVICE)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)
    
    best_val_acc = 0.0
    early_stop_counter = 0
    best_model_state = None 

    train_losses = []
    train_accuracies = []
    val_losses = []
    val_accuracies = []

    # ä» sklearn å¯¼å…¥è¯„ä¼°æŒ‡æ ‡
    from sklearn.metrics import confusion_matrix, recall_score, accuracy_score

    # åˆå§‹åŒ–æ€»è€—æ—¶
    total_start_time = time.time()

    for epoch in range(num_epochs):
        # --- å¼€å§‹è®¡æ—¶ ---
        epoch_start_time = time.time()

        # è®­ç»ƒé˜¶æ®µ
        train_loss = 0
        train_correct = 0
        train_total = 0

        model.train()
        for i, data in enumerate(train_loader):
            batch_x, batch_y = data[0], data[1]
            batch_x, batch_y = batch_x.float().to(DEVICE), batch_y.long().to(DEVICE)
            optimizer.zero_grad()
            logits, _ = model(batch_x)
            _, predicted = torch.max(logits, 1)
            loss = criterion(logits, batch_y)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
            train_correct += (predicted == batch_y).sum().item()
            train_total += batch_y.size(0)
            if (i > 0 and i % 19 == 0) or (i == len(train_loader) - 1):
                print("ç¬¬{}è½®ï¼Œç¬¬{}ä¸ªbatchï¼Œè®­ç»ƒæŸå¤±ï¼š{:.2f}ï¼Œè®­ç»ƒå‡†ç¡®ç‡ï¼š{:.2f}%".format(epoch+1, i+1, train_loss/(i+1), 100.0 * train_correct / train_total))

        # è®¡ç®—å¹¶è®°å½•è®­ç»ƒæŒ‡æ ‡
        avg_train_loss = train_loss / len(train_loader)
        train_acc = 100.0 * train_correct / train_total
        train_losses.append(avg_train_loss)
        train_accuracies.append(train_acc)

        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        # æ”¶é›†æ‰€æœ‰çœŸå®æ ‡ç­¾å’Œé¢„æµ‹ç»“æœï¼Œç”¨äº sklearn è®¡ç®—
        all_labels = []
        all_preds = []

        with torch.no_grad():
            for batch_x, batch_y in val_loader:
                batch_x, batch_y = batch_x.float().to(DEVICE), batch_y.long().to(DEVICE)
                logits, _ = model(batch_x)
                loss = criterion(logits, batch_y)
                val_loss += loss.item()

                _, predicted = torch.max(logits, 1)
                
                # æ”¶é›†åˆ° CPU åˆ—è¡¨ä¸­
                all_labels.extend(batch_y.cpu().numpy())
                all_preds.extend(predicted.cpu().numpy())

        # è®¡ç®—å¹³å‡éªŒè¯æŸå¤±
        avg_val_loss = val_loss / len(val_loader)
        # ä½¿ç”¨ sklearn è®¡ç®—å„é¡¹æŒ‡æ ‡
        val_acc = accuracy_score(all_labels, all_preds) * 100.0
        # Sensitivity (Recall for the positive class, assuming 1 is positive)
        sensitivity = recall_score(all_labels, all_preds, pos_label=1) * 100.0
        # Specificity (Recall for the negative class)
        specificity = recall_score(all_labels, all_preds, pos_label=0) * 100.0

        # è®°å½•éªŒè¯æŒ‡æ ‡ (âœ… å…³é”®ä¿®å¤)
        val_losses.append(avg_val_loss)
        val_accuracies.append(val_acc)

        # --- ç»“æŸè®¡æ—¶å¹¶è®¡ç®—è€—æ—¶ ---
        epoch_end_time = time.time()
        epoch_duration = epoch_end_time - epoch_start_time
        total_elapsed_time = epoch_end_time - total_start_time

        # æ‰“å°ç»“æœï¼ˆåŒ…å«è€—æ—¶ä¿¡æ¯ï¼‰
        print(f"éªŒè¯æŸå¤±: {avg_val_loss:.2f}, "
              f"éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%, "
              f"éªŒè¯Sensitivity: {sensitivity:.2f}%, "
              f"éªŒè¯Specificity: {specificity:.2f}% | "
              f"æœ¬è½®è€—æ—¶: {epoch_duration:.1f}s | "
              f"æ€»è€—æ—¶: {total_elapsed_time:.1f}s")

        # æ—©åœæ£€æŸ¥
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            early_stop_counter = 0
            # ä¿å­˜æœ€ä½³æ¨¡å‹å‚æ•°
            best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}
        else:
            early_stop_counter += 1
            if early_stop_counter >= patience:
                print(f"âš ï¸ æ—©åœè§¦å‘ï¼Œå½“å‰æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}% | æ€»è®­ç»ƒè€—æ—¶: {total_elapsed_time:.1f}s")
                break
    
    # ç»˜åˆ¶æŸå¤±å’Œå‡†ç¡®ç‡æ›²çº¿
    plot_loss_and_accuracy(train_losses, val_losses, train_accuracies, val_accuracies)
    print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}% | æœ¬æ¬¡è®­ç»ƒæ€»è€—æ—¶: {total_elapsed_time:.1f}s")

    # æ¢å¤æœ€ä½³æ¨¡å‹å‚æ•°
    if best_model_state is not None:
        model.load_state_dict(best_model_state)

    # æ¸…ç† GPU ç¼“å­˜ (âœ… å¢å¼ºå†…å­˜ç®¡ç†)
    torch.cuda.empty_cache()
    # åˆ é™¤ä¸å†éœ€è¦çš„å˜é‡
    del all_labels, all_preds, best_model_state
    torch.cuda.empty_cache()

    return best_val_acc

# å››ã€ä¸»æ‰§è¡Œä»£ç 

if __name__ == "__main__":
    # å¤„ç†å‘ä½œå‰æ•°æ®
    X_preictal = process_and_save_fragments(PREICTAL_PATH, "preictal")

    # å¤„ç†å‘ä½œé—´æœŸæ•°æ®
    X_interictal = process_and_save_fragments(INTERICTAL_PATH, "interictal")
     
    # ä¿è¯æ ·æœ¬æ•°1æ¯”1
    if X_preictal.shape[0] < X_interictal.shape[0]:
        print("Preictal æ ·æœ¬æ•°å°‘äº Interictal")
        X_interictal = X_interictal[:X_preictal.shape[0]]
        print(f"å·²è£å‰ª Interictal è‡³ {X_interictal.shape[0]} ä¸ªæ ·æœ¬")
    else:
        print("Interictal æ ·æœ¬æ•°å°‘äºæˆ–ç­‰äº Preictal")
        X_preictal = X_preictal[:X_interictal.shape[0]]
        print(f"å·²è£å‰ª Preictal è‡³ {X_preictal.shape[0]} ä¸ªæ ·æœ¬")
    
    print(f"æ‚£è€…ç¼–å·: CHB-{PATIENT_ID:02d}")
    print(f"Preictalæ•°æ®ç»´åº¦: {X_preictal.shape}")
    print(f"Interictalæ•°æ®ç»´åº¦: {X_interictal.shape}")
    
    # æ„å»ºæ•°æ®é›†å’Œæ ‡ç­¾
    X = np.concatenate([X_preictal, X_interictal], axis=0)
    y = np.concatenate([
        np.ones(X_preictal.shape[0]),
        np.zeros(X_interictal.shape[0])
    ], axis=0)

    print(f"\nâœ… æ•°æ®é›†æ„å»ºå®Œæˆ")
    print(f"ç‰¹å¾ X å½¢çŠ¶: {X.shape}")   
    print(f"æ ‡ç­¾ y å½¢çŠ¶: {y.shape}")  
    print(f"ç±»åˆ«åˆ†å¸ƒ: {np.bincount(y.astype(int))}")  

    # äº”æŠ˜äº¤å‰éªŒè¯
    from sklearn.model_selection import StratifiedKFold
    
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    fold_accuracies = []

    print(f"\nğŸš€ å¼€å§‹äº”æŠ˜äº¤å‰éªŒè¯...")
    
    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X, y)):
        print(f"\n--- Fold {fold_idx + 1}/5 ---")
        
        # å‡†å¤‡æ•°æ®
        X_train, y_train = X[train_idx], y[train_idx]
        X_val, y_val = X[val_idx], y[val_idx]
        
        # åˆ›å»ºDataLoader
        train_dataset = TensorDataset(
            torch.tensor(X_train, dtype=torch.float32),
            torch.tensor(y_train, dtype=torch.long)
        )
        val_dataset = TensorDataset(
            torch.tensor(X_val, dtype=torch.float32),
            torch.tensor(y_val, dtype=torch.long)
        )
        
        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
        
        print(f"è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬, éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
        
        # åˆå§‹åŒ–æ¨¡å‹
        model = MainModel(Freq=128, Time=256, num_classes=2)

        # è®­ç»ƒæ¨¡å‹
        print("å¼€å§‹è®­ç»ƒ...")
        best_acc = train_model(model, train_loader, val_loader)
        fold_accuracies.append(best_acc)

        print(f"Fold {fold_idx + 1} å‡†ç¡®ç‡: {best_acc:.2f}%")

        # æ¸…ç†GPUå†…å­˜
        del model
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    # è®¡ç®—æœ€ç»ˆç»“æœ
    mean_acc = np.mean(fold_accuracies)
    std_acc = np.std(fold_accuracies)
    
    print(f"\n=== äº”æŠ˜äº¤å‰éªŒè¯ç»“æœ ===")
    print(f"æ‚£è€… CHB{PATIENT_ID:02d}")
    print(f"å„æŠ˜å‡†ç¡®ç‡: {[f'{acc:.2f}%' for acc in fold_accuracies]}")
    print(f"å¹³å‡å‡†ç¡®ç‡: {mean_acc:.2f}% Â± {std_acc:.2f}%")
    print(f"æœ€ä½³å‡†ç¡®ç‡: {max(fold_accuracies):.2f}%")
    print(f"æœ€å·®å‡†ç¡®ç‡: {min(fold_accuracies):.2f}%")