import h5py
import numpy as np
import torch
from scipy.signal import butter, filtfilt, iirnotch
import matplotlib.pyplot as plt
from pathlib import Path
import os
from tqdm import tqdm  # æ›´å¥½çš„è¿›åº¦æ¡
import warnings
from sklearn.model_selection import StratifiedKFold
import torch
import torch.nn as nn
import torch.nn.functional as F

# å¿½ç•¥å¸¸è§è­¦å‘Š
warnings.filterwarnings("ignore")

# è®¾ç½®éšæœºç§å­
np.random.seed(42)
torch.manual_seed(42)

# -------------------------------
# é…ç½®å‚æ•°
# -------------------------------
PATIENT_ID = 23
DATA_DIR = Path("D:\\é™ˆæ•™æˆç»„\\mymodel\\data")
PREICTAL_PATH = DATA_DIR / "preictal" / f"preictal_fragments{PATIENT_ID:02d}.h5"
INTERICTAL_PATH = DATA_DIR / "interictal" / f"interictal_fragments{PATIENT_ID:02d}.h5"

# ä¿¡å·å‚æ•°
FS = 256  # é‡‡æ ·ç‡ (Hz)
N_FFT = 256
HOP_LENGTH = 128
WIN_LENGTH = 256

# è®¾å¤‡é€‰æ‹©ï¼ˆGPU åŠ é€Ÿï¼‰
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"ä½¿ç”¨è®¾å¤‡: {DEVICE}")



#ä¸€ã€æ•°æ®é¢„å¤„ç†éƒ¨åˆ†

# -------------------------------
# æ»¤æ³¢å‡½æ•°
# -------------------------------
def bandpass_filter(data, lowcut, highcut, fs, axis=-1, order=4):
    """å¸¦é€šæ»¤æ³¢å™¨ï¼ˆå¯é€‰ï¼‰"""
    nyq = 0.5 * fs
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='band')
    return filtfilt(b, a, data, axis=axis)

def notch_filter(data, freq, fs, Q=30, axis=-1):
    """å¸¦é™·æ»¤æ³¢å™¨"""
    b, a = iirnotch(freq, Q, fs)
    return filtfilt(b, a, data, axis=axis)

def remove_dc_component(data, axis=-1):
    """å»é™¤ç›´æµåˆ†é‡"""
    mean_val = np.mean(data, axis=axis, keepdims=True)
    return data - mean_val

def apply_all_filters(data, fs=FS):
    """
    åº”ç”¨æ»¤æ³¢é“¾ï¼šå·¥é¢‘é™·æ³¢ + äºŒæ¬¡è°æ³¢é™·æ³¢ + å»é™¤ DC
    è¾“å…¥: (..., channels, timepoints)
    """
    filtered = notch_filter(data, 60.0, fs=fs)
    filtered = notch_filter(filtered, 120.0, fs=fs)
    filtered = remove_dc_component(filtered)
    return filtered


def generating_raw_data_matching_stft(data_array, n_fft=N_FFT, hop_length=HOP_LENGTH, device=DEVICE):
    if data_array.size == 0:
        return np.array([])

    # è½¬ä¸º tensor å¹¶ç§»åˆ°è®¾å¤‡
    data_tensor = torch.tensor(data_array, dtype=torch.float32).to(device)  # (N, C, T)

    return data_tensor.unfold(-1, n_fft, hop_length).permute(0, 1, 3, 2)


# -------------------------------
# STFT å˜æ¢å‡½æ•°ï¼ˆä¼˜åŒ–ç‰ˆï¼Œæ”¯æŒæ‰¹é‡ + GPUï¼‰
# -------------------------------
def apply_stft_to_data(data_array, n_fft=N_FFT, hop_length=HOP_LENGTH,
                       win_length=WIN_LENGTH, fs=FS, device=DEVICE):
    """
    å¯¹ä¸‰ç»´æ•°ç»„åº”ç”¨ STFTï¼Œè¿”å›å¯¹æ•°å¹…åº¦è°±
    è¾“å…¥: (n_samples, n_channels, n_timepoints)
    è¾“å‡º: (n_samples, n_channels, freq_bins, time_frames) çš„ log-magnitude
    """
    if data_array.size == 0:
        return np.array([])

    n_samples, n_channels, n_timepoints = data_array.shape

    # è½¬ä¸º tensor å¹¶ç§»åˆ°è®¾å¤‡
    data_tensor = torch.tensor(data_array, dtype=torch.float32).to(device)  # (N, C, T)

    # é¢„å®šä¹‰çª—å‡½æ•°ï¼ˆç§»åˆ°è®¾å¤‡ï¼‰
    window = torch.hann_window(win_length).to(device)

    # æ‰¹é‡å¤„ç†æ‰€æœ‰é€šé“å’Œæ ·æœ¬ï¼ˆåˆ©ç”¨å¹¿æ’­ï¼‰
    data_flat = data_tensor.view(-1, n_timepoints)  # (N*C, T)

    # åº”ç”¨ STFTï¼ˆè¿”å›å¤æ•°ï¼‰
    stft_complex = torch.stft(
        data_flat,
        n_fft=n_fft,
        hop_length=hop_length,
        win_length=win_length,
        window=window,
        center=True,           # âœ… æ—¶é—´å¯¹é½
        return_complex=True    # âœ… å¤æ•°è¾“å‡º
    )  # shape: (N*C, F, T_frames)

    # æ¢å¤é€šé“ç»´åº¦
    freq_bins = n_fft // 2 + 1
    time_frames = stft_complex.shape[-1]
    stft_complex = stft_complex.view(n_samples, n_channels, freq_bins, time_frames)

    # åˆ é™¤ 0Hz (DC åˆ†é‡) â€”â€” åˆç†ï¼Œå°¤å…¶å¯¹ EEG
    stft_complex = stft_complex[:, :, 1:, 1:-1]  # (N, C, F-1, T')

    # è½¬ä¸ºå¯¹æ•°å¹…åº¦è°±ï¼š20 * log10(|Z| + Îµ)
    magnitude = stft_complex.abs()  # (N, C, F-1, T')
    log_magnitude_stft = 20 * torch.log10(magnitude + 1e-8)

    return log_magnitude_stft

# -------------------------------
# ä¸»å¤„ç†å‡½æ•°ï¼šé€ç‰‡æ®µå¤„ç†å¹¶ä¿å­˜
# -------------------------------
import h5py
import numpy as np
from tqdm import tqdm

def process_and_save_fragments(input_path, data_type):
    """
    é€ä¸ªåŠ è½½ HDF5 ç‰‡æ®µï¼Œæ»¤æ³¢ â†’ STFT â†’ log-magnitude
    å¹¶åœ¨ Batch ç»´åº¦ä¸Šæ‹¼æ¥æ‰€æœ‰ç‰‡æ®µçš„ç»“æœ

    Returns:
        concatenated_data: np.array, shape (Total_Samples, C, F, T)
    """
    print(f"\nğŸš€ æ­£åœ¨å¤„ç† {data_type} æ•°æ®...")

    input_list = []  # ç”¨äºæ”¶é›†æ‰€æœ‰æ¨¡å‹è¾“å…¥

    if not input_path.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
        return np.array([]), 0

    with h5py.File(input_path, 'r') as infile:
        keys = sorted(infile.keys())  # æŒ‰åç§°æ’åºï¼Œä¿è¯é¡ºåºä¸€è‡´

        for key in tqdm(keys, desc=f"{data_type.upper()} Processing", unit="frag"):
            try:
                raw_data = infile[key][()] 

                filtered_data = apply_all_filters(raw_data)

                stft_log_mag = apply_stft_to_data(filtered_data)  # (B, C, F, T)

                raw_data = generating_raw_data_matching_stft(filtered_data)

                model_input = torch.cat([stft_log_mag, raw_data], dim=2)

                model_input = model_input.cpu().numpy()  # è½¬å› CPU å’Œ NumPy

                input_list.append(model_input)

            except Exception as e:
                print(f"  âŒ å¤„ç†ç‰‡æ®µ {key} æ—¶å‡ºé”™: {e}")
                continue

    # âœ… åœ¨ Batch ç»´åº¦ (axis=0) ä¸Šæ‹¼æ¥æ‰€æœ‰ç‰‡æ®µ
    if len(input_list) == 0:
        print(f"âš ï¸  æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•ç‰‡æ®µï¼Œè¿”å›ç©ºæ•°ç»„")
        return np.array([]), 0

    concatenated_data = np.concatenate(input_list, axis=0)  # (Total_Batch, C, F, T)
    print(f"   {data_type}æ•°æ®è¾“å‡ºå½¢çŠ¶: {concatenated_data.shape}")

    return concatenated_data

# äºŒã€æ¨¡å‹éƒ¨åˆ†

class FrequencyBranch(nn.Module):
    def __init__(self, F_in=128, F_out=64):
        super().__init__()
        # åœ¨ F_in=128ï¼ˆé¢‘ç‡ç‚¹ï¼‰ä¸Šåšå±€éƒ¨å·ç§¯
        self.local_conv = nn.Sequential(
            nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1),
            nn.BatchNorm1d(16),
            nn.ReLU(),
            nn.Conv1d(16, 1, kernel_size=3, padding=1),
            nn.BatchNorm1d(1),
            nn.ReLU()
        )
        # 1x1 å·ç§¯é™ç»´ï¼šF_in â†’ F_out
        self.reduce = nn.Conv1d(F_in, F_out, kernel_size=1)

    def forward(self, x):
        # x: (B, C, F=128, T=9)
        B, C, Freq, T = x.shape

        # é‡æ’ï¼šæ¯ä¸ª (B, c, t) ç‹¬ç«‹å¤„ç†
        x = x.permute(0, 1, 3, 2).reshape(B * C * T, Freq, 1)  # (B*C*T, 128, 1)
        x = x.permute(0, 2, 1)  # â†’ (B*C*T, 1, 128) âœ… Length=128

        x = self.local_conv(x)  # (B*C*T, 1, 128) â†’ å±€éƒ¨é¢‘ç‡æ¨¡å¼

        # 1x1 å·ç§¯é™ç»´ F=128 â†’ F_out=64
        x = self.reduce(x.permute(0, 2, 1))  # â†’ (B*C*T, 128, 1) â†’ (B*C*T, 64, 1)
        x = x.permute(0, 2, 1)  # (B*C*T, 1, 64)

        # æ¢å¤å½¢çŠ¶
        x = x.reshape(B, C, T, 64).permute(0, 1, 3, 2)  # (B, C, 64, T)
        print('FrequencyBranchå®Œæˆ')
        return x  # (B, C, 64, 9)

class TCNBlock(nn.Module):
    """å•ä¸ª TCN æ®‹å·®å—ï¼ˆä¿®æ­£ç‰ˆï¼šä¿æŒåºåˆ—é•¿åº¦ï¼‰"""
    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1, downsample=False):
        super().__init__()
        self.downsample = downsample
        
        # âœ… æ­£ç¡® paddingï¼šä¿è¯è¾“å…¥è¾“å‡ºé•¿åº¦ä¸€è‡´
        self.padding = dilation * (kernel_size - 1) // 2  # ä¾‹å¦‚ dilation=2 â†’ padding=2

        # æ®‹å·®è·¯å¾„
        self.conv1 = nn.Conv1d(
            in_channels, out_channels, kernel_size,
            padding=self.padding, dilation=dilation
        )
        self.bn1 = nn.BatchNorm1d(out_channels)
        self.relu = nn.ReLU()

        self.conv2 = nn.Conv1d(
            out_channels, out_channels, kernel_size,
            padding=self.padding, dilation=dilation
        )
        self.bn2 = nn.BatchNorm1d(out_channels)

        # 1x1 å·ç§¯è°ƒæ•´æ®‹å·®ç»´åº¦
        if in_channels != out_channels or downsample:
            self.residual = nn.Conv1d(in_channels, out_channels, 1)
        else:
            self.residual = nn.Identity()

        # æœ€å¤§æ± åŒ–ç”¨äºä¸‹é‡‡æ ·ï¼ˆå¯é€‰ï¼‰
        self.pool = nn.MaxPool1d(2, stride=2) if downsample else None

    def forward(self, x):
        residual = self.residual(x)
        if self.pool:
            residual = self.pool(residual)

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)

        if self.pool:
            out = self.pool(out)

        # âœ… ç°åœ¨ out å’Œ residual é•¿åº¦ä¸€è‡´
        out += residual
        out = self.relu(out)
        print("TCNBlockå®Œæˆ")
        return out

class TimeBranch(nn.Module):
    def __init__(self, t_in=256, t_out=128):
        super().__init__()
        # TCN åœ¨ t=256 ä¸Šå»ºæ¨¡å±€éƒ¨æ—¶é—´åŠ¨æ€
        self.tcn = nn.Sequential(
            TCNBlock(in_channels=1, out_channels=32, kernel_size=3, dilation=1),
            TCNBlock(in_channels=32, out_channels=64, kernel_size=3, dilation=2),
            TCNBlock(in_channels=64, out_channels=128, kernel_size=3, dilation=4),  # æ„Ÿå—é‡å¾ˆå¤§
        )
        # TCNBlock(in_channels=128, out_channels=128, kernel_size=3, dilation=8),
        # æœ€ç»ˆ 1x1 å·ç§¯é™ç»´ï¼ˆå¯é€‰ï¼‰
        self.reduce = nn.Conv1d(128, t_out, kernel_size=1)

    def forward(self, x):
        # x: (B, C, 256, T=9)
        B, C, t, T = x.shape

        # é‡æ’ï¼šæ¯ä¸ª (B, c, t) ç‹¬ç«‹å¤„ç†
        x = x.permute(0, 1, 3, 2).reshape(B * C * T, t, 1)  # (B*C*T, 256, 1)
        x = x.permute(0, 2, 1)  # â†’ (B*C*T, 1, 256) âœ… in_channels=1, Length=256

        # TCN å»ºæ¨¡ï¼šåœ¨ t=256 ä¸Šæå–æ·±å±‚æ—¶åºç‰¹å¾
        x = self.tcn(x)  # (B*C*T, 128, 256) â†’ æ³¨æ„ï¼šé•¿åº¦ä»ä¸º 256ï¼ˆå› æœ paddingï¼‰

        # 1x1 å·ç§¯é™ç»´ï¼š128 â†’ t_out=128ï¼ˆå¯ä¿æŒï¼‰
        x = self.reduce(x)  # (B*C*T, 128, 256) â†’ (B*C*T, 128, 256)

        # å…¨å±€å¹³å‡æ± åŒ–å‹ç¼©æ—¶é—´çª—å£ï¼ˆt=256 â†’ 1ï¼‰
        x = x.mean(dim=-1, keepdim=True)  # (B*C*T, 128, 1)

        # æˆ–è€…ä½¿ç”¨ AdaptivePool:
        # x = nn.AdaptiveAvgPool1d(1)(x)  # åŒä¸Š

        # æ¢å¤å½¢çŠ¶
        x = x.reshape(B, C, T, 128).permute(0, 1, 3, 2)  # (B, C, 128, T)
        print("TimeBranchå®Œæˆ")
        return x  # (B, C, 128, 9)

class AdjacencyMatrixLearning(nn.Module):
    def __init__(self, C=22, T=9, hidden_dim=64):
        super().__init__()
        self.C = C
        self.T = T
        # æŠ•å½±ç½‘ç»œ
        self.W1 = nn.Linear(64, hidden_dim)  # freq â†’ hidden
        self.W2 = nn.Linear(64, hidden_dim)
        self.W3 = nn.Linear(128, hidden_dim) # time â†’ hidden
        self.W4 = nn.Linear(128, hidden_dim)

    def forward(self, freq_feat, time_out):
        # freq_feat: (B, C, 64, T)
        # time_out:  (B, C, 128, T)
        B, C, _, T = freq_feat.shape

        # è½¬ç½®ä¸º (B, T, C, F/t)
        freq = freq_feat.permute(0, 3, 1, 2)  # (B, T, C, 64)
        time = time_out.permute(0, 3, 1, 2)   # (B, T, C, 128)

        # æŠ•å½±åˆ°ä½ç»´ç©ºé—´
        freq_flat = freq.reshape(B * T, C, 64)
        time_flat = time.reshape(B * T, C, 128)

        # Frequency Path
        f1 = torch.relu(self.W1(freq_flat))  # (BT, C, h)
        f2 = torch.relu(self.W2(freq_flat))
        A_freq = torch.bmm(f1, f2.transpose(1, 2))  # (BT, C, C)

        # Time Path
        t1 = torch.relu(self.W3(time_flat))  # (BT, C, h)
        t2 = torch.relu(self.W4(time_flat))
        A_time = torch.bmm(t1, t2.transpose(1, 2))  # (BT, C, C)

        # èåˆ + Softmax
        A = A_freq + A_time  # (BT, C, C)
        A = A / (C ** 0.5)  # Scale
        A = torch.softmax(A, dim=-1)  # (BT, C, C)

        # æ¢å¤æ—¶é—´ç»´åº¦
        A = A.reshape(B, T, C, C)  # (B, T, C, C)
        print("AdjacencyMatrixLearningå®Œæˆ")
        return A  # åŠ¨æ€é‚»æ¥çŸ©é˜µ
    
class TemporalGCN(nn.Module):
    def __init__(self, in_channels=192, hidden_channels=128, num_layers=2, C=22):
        super().__init__()
        self.C = C
        self.num_layers = num_layers
        
        # å›¾å·ç§¯å±‚ï¼ˆ1x1 Conv å®ç° GCNï¼‰
        self.convs = nn.ModuleList()
        for _ in range(num_layers):
            # ä½¿ç”¨ 1x1 Conv æ¨¡æ‹Ÿ GCN: (B, C, F) â†’ (B, C, F')
            '''
            self.convs.append(
                nn.Conv1d(C, C, kernel_size=1)  # åœ¨èŠ‚ç‚¹ç»´åº¦ä¸Šæ“ä½œ
            )
            '''
            self.convs.append(
                nn.Linear(in_channels, hidden_channels)  # ç‰¹å¾å˜æ¢
            )
            in_channels = hidden_channels

        self.out_channels = hidden_channels

    def forward(self, x, A):
        # x: (B, T, C, F) = (B, 9, C, 192)
        # A: (B, T, C, C)
        B, T, C, Freq = x.shape

        # å­˜å‚¨æ¯ä¸€æ­¥çš„å›¾è¡¨ç¤º
        outputs = []

        for t in range(T):
            # å½“å‰æ—¶é—´æ­¥ç‰¹å¾: (B, C, F)
            xt = x[:, t, :, :]  # (B, C, F)
            At = A[:, t, :, :]  # (B, C, C)

            # GCN å±‚æ•°
            for i in range(self.num_layers):
                # 1. é‚»å±…èšåˆ: X' = A @ X
                xt = torch.bmm(At, xt)  # (B, C, F)

                # 2. ç‰¹å¾å˜æ¢ + Conv1d-like update
                conv_idx = i 
                #xt = self.convs[conv_idx](xt.transpose(1, 2)).transpose(1, 2)  # (B, C, F)
                xt = self.convs[conv_idx](xt)  # (B, C, H)

                # 3. ReLU
                xt = F.relu(xt)

            # ä¿å­˜å½“å‰æ—¶é—´æ­¥çš„å›¾è¡¨ç¤º
            outputs.append(xt)  # (B, C, H)

        # æ‹¼æ¥æ‰€æœ‰æ—¶é—´æ­¥: (B, T, C, H)
        x_gcn = torch.stack(outputs, dim=1)  # (B, T, C, H)
        x_gcn = x_gcn.mean(dim = 2)               #(B, T, H)
        print("Temporal GCNå®Œæˆ")
        return x_gcn  # (B, T, H)

class TemporalModeler(nn.Module):
    def __init__(self, input_dim, hidden_dim=128):
        super().__init__()
        self.gru = nn.GRU(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=1,
            bidirectional=True,
            batch_first=True
        )
        self.out_dim = 2 * hidden_dim  # bidirectional

    def forward(self, x):
        # x: (B, T, H)
        B, T, H = x.shape

        # Bi-GRU over time
        x, _ = self.gru(x)  # (B, T, 2*H)

        print("TemporalModelerå®Œæˆ")
        return x  # (B, T, 2H)
    
class TimeAttentionPooling(nn.Module):
    def __init__(self, in_channels):
        super().__init__()
        self.attention = nn.Sequential(
            nn.Linear(in_channels, 32),
            nn.Tanh(),
            nn.Linear(32, 1)
        )

    def forward(self, x):
        # x: (B, T, C, H)
        B, T, H = x.shape

        # æ³¨æ„åŠ›æƒé‡
        weights = self.attention(x)  # (B, T, 1)
        weights = torch.softmax(weights, dim=1)

        # åŠ æƒæ±‚å’Œ
        x = (x * weights).sum(dim=1)  # (B, H)
        print("TimeattentionPoolingå®Œæˆ")
        return x  # (B, H)
    
class DualPathModel(nn.Module):
    def __init__(self, C=22, Freq=128, t=256, T=9, num_classes=2):
        super().__init__()
        self.C, self.T = C, T

        # ç‰¹å¾æå–
        self.freq_branch = FrequencyBranch(F_in=Freq, F_out=64)
        self.time_branch = TimeBranch(t_in=t, t_out=128)

        # é‚»æ¥çŸ©é˜µå­¦ä¹ 
        self.adj_block = AdjacencyMatrixLearning(C=C, T=T)

        # ç©ºé—´å»ºæ¨¡ï¼šGCNï¼ˆæ¯æ—¶é—´æ­¥ç‹¬ç«‹ï¼‰
        self.spatial_gcn = TemporalGCN(in_channels=64+128, hidden_channels=128, C=C)

        # æ—¶é—´å»ºæ¨¡ï¼šBi-GRU
        self.temporal_rnn = TemporalModeler(input_dim=128, hidden_dim=64)

        # æ—¶é—´æ³¨æ„åŠ›æ± åŒ–
        self.time_attn_pool = TimeAttentionPooling(2 * 64)  # 2*hidden_dim

        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(64, num_classes)
        )

    def forward(self, x):
        # x: (B, C, 384, T)
        freq_feat = x[:, :, :128, :]   # (B, C, 128, 9)
        time_feat = x[:, :, 128:, :]   # (B, C, 256, 9)

        freq_out = self.freq_branch(freq_feat)  # (B, C, 64, 9)
        time_out = self.time_branch(time_feat)  # (B, C, 128, 9)

        # æ‹¼æ¥èŠ‚ç‚¹ç‰¹å¾: (B, C, 192, T)
        x_feat = torch.cat([freq_out, time_out], dim=2)
        x_feat = x_feat.permute(0, 3, 1, 2)  # (B, T, C, 192)

        # å­¦ä¹ åŠ¨æ€é‚»æ¥çŸ©é˜µ A: (B, T, C, C)
        A = self.adj_block(freq_out, time_out)

        # ç©ºé—´å»ºæ¨¡ï¼šGCN åœ¨æ¯ä¸ªæ—¶é—´æ­¥ä¸Š
        x_gcn = self.spatial_gcn(x_feat, A)  # (B, T, C, 128)

        # æ—¶é—´å»ºæ¨¡ï¼šBi-GRU åœ¨ T=9 ä¸Š
        x_temporal = self.temporal_rnn(x_gcn)  # (B, T, C, 256)

        # æ—¶é—´æ³¨æ„åŠ›æ± åŒ–: (B, T, C, 256) â†’ (B, C, 256)
        x_attn = self.time_attn_pool(x_temporal)

        # åˆ†ç±»
        logits = self.classifier(x_attn)
        
        return logits, A  # å¯è§†åŒ–é‚»æ¥çŸ©é˜µ
# -------------------------------
# æ‰§è¡Œ
# -------------------------------
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import accuracy_score

# è®­ç»ƒå‚æ•°
BATCH_SIZE = 32
LEARNING_RATE = 0.001
NUM_EPOCHS = 20
PATIENCE = 10

# -------------------------------
# è®­ç»ƒå‡½æ•°
# -------------------------------
def train_model(model, train_loader, val_loader, num_epochs=NUM_EPOCHS, patience=PATIENCE):
    """ç®€å•çš„è®­ç»ƒå‡½æ•°ï¼Œåªè¿”å›æœ€ä½³éªŒè¯å‡†ç¡®ç‡"""
    model = model.to(DEVICE)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)
    
    best_val_acc = 0.0
    early_stop_counter = 0
    
    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        i = 0
        for batch_x, batch_y in train_loader:
            print(f"Epoch {epoch+1}, Batch {i+1}")
            i += 1
            batch_x, batch_y = batch_x.float().to(DEVICE), batch_y.long().to(DEVICE)
            
            optimizer.zero_grad()
            logits, _ = model(batch_x)
            loss = criterion(logits, batch_y)
            loss.backward()
            optimizer.step()
            if i == 3:
                break
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for batch_x, batch_y in val_loader:
                batch_x, batch_y = batch_x.float().to(DEVICE), batch_y.long().to(DEVICE)
                logits, _ = model(batch_x)
                _, predicted = torch.max(logits, 1)
                val_total += batch_y.size(0)
                val_correct += (predicted == batch_y).sum().item()
        
        val_acc = 100.0 * val_correct / val_total
        print(f"éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%")

        # æ—©åœæ£€æŸ¥
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            early_stop_counter = 0
        else:
            early_stop_counter += 1
            if early_stop_counter >= patience:
                break
    
    return best_val_acc

def evaluate_model(model, test_loader):
    """è¯„ä¼°æ¨¡å‹ï¼Œè¿”å›å‡†ç¡®ç‡"""
    model.eval()
    correct = 0
    total = 0
    
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            batch_x, batch_y = batch_x.float().to(DEVICE), batch_y.long().to(DEVICE)
            logits, _ = model(batch_x)
            _, predicted = torch.max(logits, 1)
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()
    
    return 100.0 * correct / total

# -------------------------------
# ä¸»æ‰§è¡Œä»£ç 
# -------------------------------
if __name__ == "__main__":
    # å¤„ç†å‘ä½œå‰æ•°æ®
    X_preictal = process_and_save_fragments(PREICTAL_PATH, "preictal")

    # å¤„ç†å‘ä½œé—´æœŸæ•°æ®
    X_interictal = process_and_save_fragments(INTERICTAL_PATH, "interictal")
     
    # ä¿è¯æ ·æœ¬æ•°1æ¯”1
    if X_preictal.shape[0] < X_interictal.shape[0]:
        print("Preictal æ ·æœ¬æ•°å°‘äº Interictal")
        X_interictal = X_interictal[:X_preictal.shape[0]]
        print(f"å·²è£å‰ª Interictal è‡³ {X_interictal.shape[0]} ä¸ªæ ·æœ¬")
    else:
        print("Interictal æ ·æœ¬æ•°å°‘äºæˆ–ç­‰äº Preictal")
        X_preictal = X_preictal[:X_interictal.shape[0]]
        print(f"å·²è£å‰ª Preictal è‡³ {X_preictal.shape[0]} ä¸ªæ ·æœ¬")
    
    print(f"æ‚£è€…ç¼–å·: CHB-{PATIENT_ID:02d}")
    print(f"Preictalæ•°æ®ç»´åº¦: {X_preictal.shape}")
    print(f"Interictalæ•°æ®ç»´åº¦: {X_interictal.shape}")
    
    # æ„å»ºæ•°æ®é›†å’Œæ ‡ç­¾
    X = np.concatenate([X_preictal, X_interictal], axis=0)
    y = np.concatenate([
        np.ones(X_preictal.shape[0]),
        np.zeros(X_interictal.shape[0])
    ], axis=0)

    print(f"\nâœ… æ•°æ®é›†æ„å»ºå®Œæˆ")
    print(f"ç‰¹å¾ X å½¢çŠ¶: {X.shape}")   
    print(f"æ ‡ç­¾ y å½¢çŠ¶: {y.shape}")  
    print(f"ç±»åˆ«åˆ†å¸ƒ: {np.bincount(y.astype(int))}")  

    # äº”æŠ˜äº¤å‰éªŒè¯
    from sklearn.model_selection import StratifiedKFold
    
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    fold_accuracies = []

    print(f"\nğŸš€ å¼€å§‹äº”æŠ˜äº¤å‰éªŒè¯...")
    
    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X, y)):
        print(f"\n--- Fold {fold_idx + 1}/5 ---")
        
        # å‡†å¤‡æ•°æ®
        X_train, y_train = X[train_idx], y[train_idx]
        X_val, y_val = X[val_idx], y[val_idx]
        
        # åˆ›å»ºDataLoader
        train_dataset = TensorDataset(
            torch.tensor(X_train, dtype=torch.float32),
            torch.tensor(y_train, dtype=torch.long)
        )
        val_dataset = TensorDataset(
            torch.tensor(X_val, dtype=torch.float32),
            torch.tensor(y_val, dtype=torch.long)
        )
        
        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
        
        print(f"è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬, éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
        
        # åˆå§‹åŒ–æ¨¡å‹
        model = DualPathModel(C=22, Freq=128, t=256, T=9, num_classes=2)
        
        # è®­ç»ƒæ¨¡å‹
        print("å¼€å§‹è®­ç»ƒ...")
        best_acc = train_model(model, train_loader, val_loader)
        
        # æœ€ç»ˆè¯„ä¼°
        final_acc = evaluate_model(model, val_loader)
        fold_accuracies.append(final_acc)
        
        print(f"Fold {fold_idx + 1} å‡†ç¡®ç‡: {final_acc:.2f}%")
        
        # æ¸…ç†GPUå†…å­˜
        del model
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    # è®¡ç®—æœ€ç»ˆç»“æœ
    mean_acc = np.mean(fold_accuracies)
    std_acc = np.std(fold_accuracies)
    
    print(f"\nğŸ¯ === äº”æŠ˜äº¤å‰éªŒè¯ç»“æœ ===")
    print(f"æ‚£è€… CHB{PATIENT_ID:02d}")
    print(f"å„æŠ˜å‡†ç¡®ç‡: {[f'{acc:.2f}%' for acc in fold_accuracies]}")
    print(f"å¹³å‡å‡†ç¡®ç‡: {mean_acc:.2f}% Â± {std_acc:.2f}%")
    print(f"æœ€ä½³å‡†ç¡®ç‡: {max(fold_accuracies):.2f}%")
    print(f"æœ€å·®å‡†ç¡®ç‡: {min(fold_accuracies):.2f}%")